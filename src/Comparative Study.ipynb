{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6284f622",
   "metadata": {},
   "source": [
    "# Comparative Study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1699def6",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c016f2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from feature_engineering import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1018ebfb",
   "metadata": {},
   "source": [
    "## Defining algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7d9cf8",
   "metadata": {},
   "source": [
    "### Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "037e98d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, lr=0.001, epochs=10):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.weights = None\n",
    "\n",
    "    def train(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            y_h=np.dot(X,self.weights)\n",
    "            for idx, x_i in X.iterrows():\n",
    "                y_hat = np.dot(x_i, self.weights)\n",
    "                if y_hat*y[idx]<=0:\n",
    "                    self.weights += x_i*y[idx]\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_hat=np.dot(X, self.weights)\n",
    "        return np.where(y_hat >= 0, 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a1b42e",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5143828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionGD:\n",
    "    def __init__(self, learning_rate=0.01, n_iters=1000, random_state=None,threshold=0.5):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.random_state = random_state\n",
    "        self.threshold=threshold\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        self.costs = []\n",
    "        \n",
    "        # set random seed for reproducibility\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "        \n",
    "        # gradient descent\n",
    "        for i in range(self.n_iters):\n",
    "            # calculate predicted probabilities and gradients\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_pred = self.sigmoid(linear_model)\n",
    "            dw = np.dot(X.T, (y_pred - y)) / n_samples\n",
    "            db = np.sum(y_pred - y) / n_samples\n",
    "            \n",
    "            # update weights and bias\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            \n",
    "            # calculate cost and add to list for graphing\n",
    "            y_pred = self.sigmoid(np.dot(X, self.weights) + self.bias)\n",
    "            y_pred[y_pred == 0] = 1e-15  # add small constant value to avoid NaN in cost\n",
    "            y_pred[y_pred == 1] = 1 - 1e-15  # add small constant value to avoid NaN in cost\n",
    "            cost = -1/n_samples * np.sum(y * np.log(y_pred) + (1-y) * np.log(1-y_pred))\n",
    "            self.costs.append(cost)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_pred = self.sigmoid(linear_model)\n",
    "        y_pred_class = [1 if i > self.threshold else 0 for i in y_pred]\n",
    "        return y_pred_class\n",
    "    \n",
    "    def plot_cost(self):\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        plt.plot(np.arange(1, len(self.costs)+1), self.costs)\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Cost')\n",
    "        plt.title('Gradient Descent Cost Graph')\n",
    "        plt.show()\n",
    "        fig.savefig('unormalizedLR_graphs/'+\"GD\"+str(len(str(self.learning_rate)))+str(self.threshold)[-1]+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba32735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionSGD:\n",
    "    def __init__(self, learning_rate=0.01, n_iters=1000, batch_size=1, random_state=None,threshold=0.5):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.batch_size = batch_size\n",
    "        self.random_state = random_state\n",
    "        self.threshold=threshold\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        self.costs = []\n",
    "        \n",
    "        # set random seed for reproducibility\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "        \n",
    "        # stochastic gradient descent\n",
    "        for i in range(self.n_iters):\n",
    "            # shuffle data\n",
    "            idx = np.arange(n_samples)\n",
    "            np.random.shuffle(idx)\n",
    "            X_shuffled = X[idx]\n",
    "            y_shuffled = y[idx]\n",
    "            \n",
    "            # loop over batches\n",
    "            for j in range(0, n_samples, self.batch_size):\n",
    "                # get mini-batch\n",
    "                X_batch = X_shuffled[j:j+self.batch_size]\n",
    "                y_batch = y_shuffled[j:j+self.batch_size]\n",
    "                \n",
    "                # calculate predicted probabilities and gradients\n",
    "                linear_model = np.dot(X_batch, self.weights) + self.bias\n",
    "                y_pred = self.sigmoid(linear_model)\n",
    "                dw = np.dot(X_batch.T, (y_pred - y_batch)) / self.batch_size\n",
    "                db = np.sum(y_pred - y_batch) / self.batch_size\n",
    "                \n",
    "                # update weights and bias\n",
    "                self.weights -= self.learning_rate * dw\n",
    "                self.bias -= self.learning_rate * db\n",
    "            \n",
    "            # calculate cost and add to list for graphing\n",
    "            y_pred = self.sigmoid(np.dot(X, self.weights) + self.bias)\n",
    "            y_pred[y_pred == 0] = 1e-15  # add small constant value to avoid NaN in cost\n",
    "            y_pred[y_pred == 1] = 1 - 1e-15  # add small constant value to avoid NaN in cost\n",
    "            cost = -1/n_samples * np.sum(y * np.log(y_pred) + (1-y) * np.log(1-y_pred))\n",
    "            self.costs.append(cost)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_pred = self.sigmoid(linear_model)\n",
    "        return y_pred\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred_proba = self.predict_proba(X)\n",
    "        y_pred_class = [1 if i > self.threshold else 0 for i in y_pred_proba]\n",
    "        return y_pred_class\n",
    "    \n",
    "    def plot_cost(self):\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        plt.plot(np.arange(1, len(self.costs)+1), self.costs)\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Cost')\n",
    "        plt.title('Stochastic Gradient Descent Cost Graph')\n",
    "        plt.show()\n",
    "        fig.savefig('unormalizedLR_graphs/'+\"SGD\"+str(len(str(self.learning_rate)))+str(self.threshold)[-1]+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5686ed97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionMiniBatchGD:\n",
    "    def __init__(self, learning_rate=0.01, n_iters=1000, batch_size=32, random_state=None,threshold=0.5):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.batch_size = batch_size\n",
    "        self.random_state = random_state\n",
    "        self.threshold=threshold\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # initialize weights and bias to zero\n",
    "        self.weights = np.zeros(X.shape[1])\n",
    "        self.bias = 0\n",
    "        \n",
    "        # initialize costs list for storing costs at each iteration\n",
    "        self.costs = []\n",
    "        \n",
    "        # set random seed for reproducibility\n",
    "        n_samples, n_features = X.shape\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "            \n",
    "        # minibatch gradient descent\n",
    "        for i in range(self.n_iters):\n",
    "            # shuffle data\n",
    "            idx = np.arange(X.shape[0])\n",
    "            np.random.shuffle(idx)\n",
    "            X = X[idx]\n",
    "            y = y[idx]\n",
    "            \n",
    "            # loop over batches\n",
    "            for j in range(0, X.shape[0], self.batch_size):\n",
    "                # get minibatch\n",
    "                X_batch = X[j:j+self.batch_size]\n",
    "                y_batch = y[j:j+self.batch_size]\n",
    "                \n",
    "                # calculate predicted probabilities and gradients\n",
    "                y_pred = self.sigmoid(np.dot(X_batch, self.weights) + self.bias)\n",
    "                dw = np.dot(X_batch.T, (y_pred - y_batch)) / self.batch_size\n",
    "                db = np.sum(y_pred - y_batch) / self.batch_size\n",
    "                \n",
    "                # update weights and bias\n",
    "                self.weights -= self.learning_rate * dw\n",
    "                self.bias -= self.learning_rate * db\n",
    "            \n",
    "            # calculate cost and add to list for graphing\n",
    "            y_pred = self.sigmoid(np.dot(X, self.weights) + self.bias)\n",
    "            y_pred[y_pred == 0] = 1e-15  # add small constant value to avoid NaN in cost\n",
    "            y_pred[y_pred == 1] = 1 - 1e-15  # add small constant value to avoid NaN in cost\n",
    "            cost = -1/n_samples * np.sum(y * np.log(y_pred) + (1-y) * np.log(1-y_pred))\n",
    "            self.costs.append(cost)\n",
    "            \n",
    "        return self\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        return self.sigmoid(np.dot(X, self.weights) + self.bias)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.where(self.predict_proba(X) > self.threshold, 1, 0)\n",
    "    \n",
    "    def plot_cost(self):\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        plt.plot(range(1, len(self.costs) + 1), self.costs)\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Cost')\n",
    "        plt.title('Logistic Regression Cost Graph')\n",
    "        plt.show()\n",
    "        fig.savefig('unormalizedLR_graphs/'+\"minibatchGD\"+str(len(str(self.learning_rate)))+str(self.threshold)[-1]+\".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15cc999",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc84ab6",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5b78725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of random test: 1\n",
      "FLDM1: 4.787234042553192%\n",
      "47     1\n",
      "472   -1\n",
      "302    1\n",
      "405   -1\n",
      "120   -1\n",
      "      ..\n",
      "332   -1\n",
      "269   -1\n",
      "337    1\n",
      "91     1\n",
      "80    -1\n",
      "Name: diagnosis, Length: 188, dtype: int64\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input y_true contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     51\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     52\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m---> 53\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy_GD (0.4, 0.001):\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m## With SGD and threshold as 0.5 and learning reate=0.001\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python-datascience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:211\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \n\u001b[0;32m    147\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 211\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python-datascience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:85\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03my_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     84\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[1;32m---> 85\u001b[0m type_true \u001b[38;5;241m=\u001b[39m \u001b[43mtype_of_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my_true\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     88\u001b[0m y_type \u001b[38;5;241m=\u001b[39m {type_true, type_pred}\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python-datascience\\lib\\site-packages\\sklearn\\utils\\multiclass.py:332\u001b[0m, in \u001b[0;36mtype_of_target\u001b[1;34m(y, input_name)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# check float and contains non-integer float values\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(y \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)):\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;66;03m# [.1, .2, 3] or [[.1, .2, 3]] or [[1., .2]] and not [1., 2., 3.]\u001b[39;00m\n\u001b[1;32m--> 332\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontinuous\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m suffix\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python-datascience\\lib\\site-packages\\sklearn\\utils\\validation.py:146\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    125\u001b[0m             \u001b[38;5;129;01mnot\u001b[39;00m allow_nan\n\u001b[0;32m    126\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m estimator_name\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    131\u001b[0m             \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    132\u001b[0m             msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    133\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    144\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    145\u001b[0m             )\n\u001b[1;32m--> 146\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nan:\n",
      "\u001b[1;31mValueError\u001b[0m: Input y_true contains NaN."
     ]
    }
   ],
   "source": [
    "for i in range(1, 11):\n",
    "    df = pd.read_csv(\"../data/dataset1.csv\")\n",
    "    df=df.drop('id',axis=1)\n",
    "    df_imputed = df.copy()\n",
    "    mean = np.mean(df_imputed, axis=0)\n",
    "    for j in range(1, df_imputed.shape[1]):\n",
    "        df_imputed.iloc[:, j].fillna(mean[j-1], inplace=True)\n",
    "    print(f\"Result of random test: {i}\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_imputed, shuffle = True, random_state = 11*i)\n",
    "    \n",
    "    # Models that do not require normalisation\n",
    "    # PM1\n",
    "#     PM1 = Perceptron(lr = 0.01, epochs = 500)\n",
    "#     PM1.train(X_train, y_train)\n",
    "#     y_pred1 = PM1.predict(X_test)\n",
    "#     accuracy_pm1 = evaluate(y_test, y_pred1, verbose = False)\n",
    "#     print(f\"PM1: {accuracy_pm1}%\")\n",
    "    # FLDM1\n",
    "    fldm = LinearDiscriminantAnalysis(n_components=1)\n",
    "    fldm.fit(X_train, y_train)\n",
    "    # project the training data onto the 1-dimensional FLDM space\n",
    "    X_train_lda = fldm.transform(X_train)\n",
    "    mean_pos = np.mean(X_train_lda[y_train == 1])\n",
    "    mean_neg = np.mean(X_train_lda[y_train == -1])\n",
    "    std_pos = np.std(X_train_lda[y_train == 1])\n",
    "    std_neg = np.std(X_train_lda[y_train == -1])\n",
    "    threshold = (mean_pos + mean_neg) / 2\n",
    "    X_test_lda = fldm.transform(X_test)\n",
    "    # evaluate the performance of the model on the testing data\n",
    "    y_pred = np.where(X_test_lda > threshold, 1, -1)\n",
    "    accuracy_fldm1 = evaluate(y_test, y_pred, verbose = False)\n",
    "    print(f\"FLDM1: {accuracy_fldm1}%\")\n",
    "    \n",
    "    # LR1\n",
    "    df = pd.read_csv(\"../data/dataset1.csv\")\n",
    "    df = df.drop('id',axis=1)\n",
    "    df_imputed = df.copy()\n",
    "    mean = np.mean(df_imputed, axis=0)\n",
    "    for j in range(1, df_imputed.shape[1]):\n",
    "        df_imputed.iloc[:, j].fillna(mean[j-1], inplace=True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_imputed, shuffle = True, random_state = 11*i)\n",
    "    print(y_test)\n",
    "    y_train = y_train.map({\"M\": 1, \"B\": 0})\n",
    "    y_test = y_test.map({\"M\": 1, \"B\": 0})\n",
    "    X_train = X_train.to_numpy()\n",
    "    y_train = y_train.to_numpy()\n",
    "    X_test = X_test.to_numpy()\n",
    "    y_test = y_test.to_numpy()\n",
    "    ## With gradient descent and threshold as 0.4 and learning reate=0.001\n",
    "    model = LogisticRegressionGD(learning_rate=0.001, n_iters=1000,threshold=0.4)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy_GD (0.4, 0.001):\", accuracy)\n",
    "    ## With SGD and threshold as 0.5 and learning reate=0.001\n",
    "    model = LogisticRegressionSGD(learning_rate=0.001, n_iters=1000,threshold=0.5)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = evaluate(y_test, y_pred, verbose = False)\n",
    "    print(\"Accuracy_SGD (0.5, 0.001):\", accuracy)\n",
    "    ## With gradient descent and threshold as 0.5 and learning reate=0.0001\n",
    "    model = LogisticRegressionMiniBatchGD(learning_rate=0.0001, n_iters=1000,threshold=0.5)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = evaluate(y_test, y_pred, verbose = False)\n",
    "    print(\"Accuracy_MGD (0.5, 0.0001):\", accuracy)\n",
    "    \n",
    "    # Normalised\n",
    "    mean = np.mean(X, axis=0)\n",
    "    stddev = np.std(X, axis=0)\n",
    "    y = df_imputed['diagnosis']\n",
    "    X = df_imputed.drop(['diagnosis'], axis = 1)\n",
    "    X_normalised = (X.copy() - mean) / stddev\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_imputed, shuffle = True, random_state = 11*i)\n",
    "    \n",
    "    print(\"------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d40895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c810c33e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
