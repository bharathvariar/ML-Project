{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "233dbdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3186a0",
   "metadata": {},
   "source": [
    "## Two Layer net with 50 nodes in each and tanh() as activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cb335a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/629], Loss: 0.6267\n",
      "Epoch [1/10], Step [200/629], Loss: 0.6351\n",
      "Epoch [1/10], Step [300/629], Loss: 0.4902\n",
      "Epoch [1/10], Step [400/629], Loss: 0.2730\n",
      "Epoch [1/10], Step [500/629], Loss: 0.4448\n",
      "Epoch [1/10], Step [600/629], Loss: 0.2097\n",
      "Epoch [2/10], Step [100/629], Loss: 0.3631\n",
      "Epoch [2/10], Step [200/629], Loss: 0.0926\n",
      "Epoch [2/10], Step [300/629], Loss: 0.1667\n",
      "Epoch [2/10], Step [400/629], Loss: 0.2148\n",
      "Epoch [2/10], Step [500/629], Loss: 0.1321\n",
      "Epoch [2/10], Step [600/629], Loss: 0.2322\n",
      "Epoch [3/10], Step [100/629], Loss: 0.2155\n",
      "Epoch [3/10], Step [200/629], Loss: 0.1142\n",
      "Epoch [3/10], Step [300/629], Loss: 0.2106\n",
      "Epoch [3/10], Step [400/629], Loss: 0.1521\n",
      "Epoch [3/10], Step [500/629], Loss: 0.1937\n",
      "Epoch [3/10], Step [600/629], Loss: 0.1423\n",
      "Epoch [4/10], Step [100/629], Loss: 0.1690\n",
      "Epoch [4/10], Step [200/629], Loss: 0.1110\n",
      "Epoch [4/10], Step [300/629], Loss: 0.5330\n",
      "Epoch [4/10], Step [400/629], Loss: 0.0918\n",
      "Epoch [4/10], Step [500/629], Loss: 0.1202\n",
      "Epoch [4/10], Step [600/629], Loss: 0.0669\n",
      "Epoch [5/10], Step [100/629], Loss: 0.0446\n",
      "Epoch [5/10], Step [200/629], Loss: 0.0970\n",
      "Epoch [5/10], Step [300/629], Loss: 0.1019\n",
      "Epoch [5/10], Step [400/629], Loss: 0.1625\n",
      "Epoch [5/10], Step [500/629], Loss: 0.1197\n",
      "Epoch [5/10], Step [600/629], Loss: 0.1545\n",
      "Epoch [6/10], Step [100/629], Loss: 0.0907\n",
      "Epoch [6/10], Step [200/629], Loss: 0.1118\n",
      "Epoch [6/10], Step [300/629], Loss: 0.0679\n",
      "Epoch [6/10], Step [400/629], Loss: 0.0759\n",
      "Epoch [6/10], Step [500/629], Loss: 0.0815\n",
      "Epoch [6/10], Step [600/629], Loss: 0.0738\n",
      "Epoch [7/10], Step [100/629], Loss: 0.1027\n",
      "Epoch [7/10], Step [200/629], Loss: 0.1155\n",
      "Epoch [7/10], Step [300/629], Loss: 0.0683\n",
      "Epoch [7/10], Step [400/629], Loss: 0.1073\n",
      "Epoch [7/10], Step [500/629], Loss: 0.0339\n",
      "Epoch [7/10], Step [600/629], Loss: 0.0539\n",
      "Epoch [8/10], Step [100/629], Loss: 0.0742\n",
      "Epoch [8/10], Step [200/629], Loss: 0.2457\n",
      "Epoch [8/10], Step [300/629], Loss: 0.0648\n",
      "Epoch [8/10], Step [400/629], Loss: 0.0804\n",
      "Epoch [8/10], Step [500/629], Loss: 0.0804\n",
      "Epoch [8/10], Step [600/629], Loss: 0.0607\n",
      "Epoch [9/10], Step [100/629], Loss: 0.0622\n",
      "Epoch [9/10], Step [200/629], Loss: 0.0814\n",
      "Epoch [9/10], Step [300/629], Loss: 0.0911\n",
      "Epoch [9/10], Step [400/629], Loss: 0.1424\n",
      "Epoch [9/10], Step [500/629], Loss: 0.0811\n",
      "Epoch [9/10], Step [600/629], Loss: 0.0831\n",
      "Epoch [10/10], Step [100/629], Loss: 0.0788\n",
      "Epoch [10/10], Step [200/629], Loss: 0.1067\n",
      "Epoch [10/10], Step [300/629], Loss: 0.0123\n",
      "Epoch [10/10], Step [400/629], Loss: 0.0200\n",
      "Epoch [10/10], Step [500/629], Loss: 0.1333\n",
      "Epoch [10/10], Step [600/629], Loss: 0.0673\n",
      "Test Accuracy: 96.41%\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 50]          39,250\n",
      "              Tanh-2                   [-1, 50]               0\n",
      "            Linear-3                   [-1, 10]             510\n",
      "================================================================\n",
      "Total params: 39,760\n",
      "Trainable params: 39,760\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.15\n",
      "Estimated Total Size (MB): 0.16\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set the device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 28 * 28  # MNIST image size is 28x28\n",
    "hidden_size = 50\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = MNIST(root='data/', train=True, transform=ToTensor(), download=True)\n",
    "\n",
    "# Split dataset into train and test\n",
    "train_size = int(0.67 * len(train_dataset))\n",
    "test_size = len(train_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(train_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the neural network architecture\n",
    "class TwoLayerNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.activation(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "model = TwoLayerNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Store accuracy in the dictionary with the three-tuple as the key\n",
    "accuracies = {(2, hidden_size, 't'): accuracy}\n",
    "\n",
    "# Print the summary of the model\n",
    "summary(model, (input_size,))\n",
    "\n",
    "# Plot accuracies\n",
    "labels_graph = ['2 Layers, 50 Nodes, Tanh']\n",
    "accuracies_values = [accuracy]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0592477",
   "metadata": {},
   "source": [
    "## Two Layer net with 60 nodes in each and tanh() as activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83fa7931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/629], Loss: 0.6365\n",
      "Epoch [1/10], Step [200/629], Loss: 0.5250\n",
      "Epoch [1/10], Step [300/629], Loss: 0.2632\n",
      "Epoch [1/10], Step [400/629], Loss: 0.4419\n",
      "Epoch [1/10], Step [500/629], Loss: 0.2265\n",
      "Epoch [1/10], Step [600/629], Loss: 0.2866\n",
      "Epoch [2/10], Step [100/629], Loss: 0.3093\n",
      "Epoch [2/10], Step [200/629], Loss: 0.2990\n",
      "Epoch [2/10], Step [300/629], Loss: 0.3896\n",
      "Epoch [2/10], Step [400/629], Loss: 0.1071\n",
      "Epoch [2/10], Step [500/629], Loss: 0.2582\n",
      "Epoch [2/10], Step [600/629], Loss: 0.0747\n",
      "Epoch [3/10], Step [100/629], Loss: 0.2092\n",
      "Epoch [3/10], Step [200/629], Loss: 0.1306\n",
      "Epoch [3/10], Step [300/629], Loss: 0.2022\n",
      "Epoch [3/10], Step [400/629], Loss: 0.0936\n",
      "Epoch [3/10], Step [500/629], Loss: 0.2348\n",
      "Epoch [3/10], Step [600/629], Loss: 0.1449\n",
      "Epoch [4/10], Step [100/629], Loss: 0.1174\n",
      "Epoch [4/10], Step [200/629], Loss: 0.0889\n",
      "Epoch [4/10], Step [300/629], Loss: 0.0425\n",
      "Epoch [4/10], Step [400/629], Loss: 0.1806\n",
      "Epoch [4/10], Step [500/629], Loss: 0.0608\n",
      "Epoch [4/10], Step [600/629], Loss: 0.0744\n",
      "Epoch [5/10], Step [100/629], Loss: 0.1736\n",
      "Epoch [5/10], Step [200/629], Loss: 0.1752\n",
      "Epoch [5/10], Step [300/629], Loss: 0.0285\n",
      "Epoch [5/10], Step [400/629], Loss: 0.0782\n",
      "Epoch [5/10], Step [500/629], Loss: 0.1917\n",
      "Epoch [5/10], Step [600/629], Loss: 0.1294\n",
      "Epoch [6/10], Step [100/629], Loss: 0.0265\n",
      "Epoch [6/10], Step [200/629], Loss: 0.0271\n",
      "Epoch [6/10], Step [300/629], Loss: 0.0616\n",
      "Epoch [6/10], Step [400/629], Loss: 0.1885\n",
      "Epoch [6/10], Step [500/629], Loss: 0.1258\n",
      "Epoch [6/10], Step [600/629], Loss: 0.1528\n",
      "Epoch [7/10], Step [100/629], Loss: 0.1794\n",
      "Epoch [7/10], Step [200/629], Loss: 0.0413\n",
      "Epoch [7/10], Step [300/629], Loss: 0.1009\n",
      "Epoch [7/10], Step [400/629], Loss: 0.0545\n",
      "Epoch [7/10], Step [500/629], Loss: 0.0533\n",
      "Epoch [7/10], Step [600/629], Loss: 0.0551\n",
      "Epoch [8/10], Step [100/629], Loss: 0.0582\n",
      "Epoch [8/10], Step [200/629], Loss: 0.0183\n",
      "Epoch [8/10], Step [300/629], Loss: 0.0615\n",
      "Epoch [8/10], Step [400/629], Loss: 0.0464\n",
      "Epoch [8/10], Step [500/629], Loss: 0.1743\n",
      "Epoch [8/10], Step [600/629], Loss: 0.0616\n",
      "Epoch [9/10], Step [100/629], Loss: 0.1408\n",
      "Epoch [9/10], Step [200/629], Loss: 0.0955\n",
      "Epoch [9/10], Step [300/629], Loss: 0.0095\n",
      "Epoch [9/10], Step [400/629], Loss: 0.0474\n",
      "Epoch [9/10], Step [500/629], Loss: 0.0283\n",
      "Epoch [9/10], Step [600/629], Loss: 0.0382\n",
      "Epoch [10/10], Step [100/629], Loss: 0.0154\n",
      "Epoch [10/10], Step [200/629], Loss: 0.1860\n",
      "Epoch [10/10], Step [300/629], Loss: 0.0249\n",
      "Epoch [10/10], Step [400/629], Loss: 0.0789\n",
      "Epoch [10/10], Step [500/629], Loss: 0.0760\n",
      "Epoch [10/10], Step [600/629], Loss: 0.1127\n",
      "Test Accuracy: 96.63%\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 60]          47,100\n",
      "              Tanh-2                   [-1, 60]               0\n",
      "            Linear-3                   [-1, 10]             610\n",
      "================================================================\n",
      "Total params: 47,710\n",
      "Trainable params: 47,710\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.18\n",
      "Estimated Total Size (MB): 0.19\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set the device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 28 * 28  # MNIST image size is 28x28\n",
    "hidden_size = 60\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = MNIST(root='data/', train=True, transform=ToTensor(), download=True)\n",
    "\n",
    "# Split dataset into train and test\n",
    "train_size = int(0.67 * len(train_dataset))\n",
    "test_size = len(train_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(train_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the neural network architecture\n",
    "class TwoLayerNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.activation(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "model = TwoLayerNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Store accuracy in the dictionary with the three-tuple as the key\n",
    "accuracies = {(2, hidden_size, 't'): accuracy}\n",
    "\n",
    "# Print the summary of the model\n",
    "summary(model, (input_size,))\n",
    "\n",
    "# Plot accuracies\n",
    "if not isinstance(labels_graph, list):\n",
    "    labels_graph = labels_graph.tolist()\n",
    "labels_graph.append('2 Layers, 60 Nodes, Tanh')\n",
    "accuracies_values.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05073a55",
   "metadata": {},
   "source": [
    "## Two Layer net with 75 nodes in each and tanh() as activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "900de5a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/629], Loss: 0.6570\n",
      "Epoch [1/10], Step [200/629], Loss: 0.3882\n",
      "Epoch [1/10], Step [300/629], Loss: 0.5006\n",
      "Epoch [1/10], Step [400/629], Loss: 0.3554\n",
      "Epoch [1/10], Step [500/629], Loss: 0.2201\n",
      "Epoch [1/10], Step [600/629], Loss: 0.2600\n",
      "Epoch [2/10], Step [100/629], Loss: 0.2338\n",
      "Epoch [2/10], Step [200/629], Loss: 0.2279\n",
      "Epoch [2/10], Step [300/629], Loss: 0.2559\n",
      "Epoch [2/10], Step [400/629], Loss: 0.1767\n",
      "Epoch [2/10], Step [500/629], Loss: 0.1430\n",
      "Epoch [2/10], Step [600/629], Loss: 0.2399\n",
      "Epoch [3/10], Step [100/629], Loss: 0.1151\n",
      "Epoch [3/10], Step [200/629], Loss: 0.0674\n",
      "Epoch [3/10], Step [300/629], Loss: 0.2233\n",
      "Epoch [3/10], Step [400/629], Loss: 0.1686\n",
      "Epoch [3/10], Step [500/629], Loss: 0.1091\n",
      "Epoch [3/10], Step [600/629], Loss: 0.1443\n",
      "Epoch [4/10], Step [100/629], Loss: 0.1678\n",
      "Epoch [4/10], Step [200/629], Loss: 0.0697\n",
      "Epoch [4/10], Step [300/629], Loss: 0.1350\n",
      "Epoch [4/10], Step [400/629], Loss: 0.1311\n",
      "Epoch [4/10], Step [500/629], Loss: 0.0924\n",
      "Epoch [4/10], Step [600/629], Loss: 0.1404\n",
      "Epoch [5/10], Step [100/629], Loss: 0.1317\n",
      "Epoch [5/10], Step [200/629], Loss: 0.0503\n",
      "Epoch [5/10], Step [300/629], Loss: 0.0435\n",
      "Epoch [5/10], Step [400/629], Loss: 0.0601\n",
      "Epoch [5/10], Step [500/629], Loss: 0.0463\n",
      "Epoch [5/10], Step [600/629], Loss: 0.1192\n",
      "Epoch [6/10], Step [100/629], Loss: 0.0816\n",
      "Epoch [6/10], Step [200/629], Loss: 0.1017\n",
      "Epoch [6/10], Step [300/629], Loss: 0.1132\n",
      "Epoch [6/10], Step [400/629], Loss: 0.0427\n",
      "Epoch [6/10], Step [500/629], Loss: 0.0374\n",
      "Epoch [6/10], Step [600/629], Loss: 0.0854\n",
      "Epoch [7/10], Step [100/629], Loss: 0.0560\n",
      "Epoch [7/10], Step [200/629], Loss: 0.0456\n",
      "Epoch [7/10], Step [300/629], Loss: 0.1020\n",
      "Epoch [7/10], Step [400/629], Loss: 0.0234\n",
      "Epoch [7/10], Step [500/629], Loss: 0.0274\n",
      "Epoch [7/10], Step [600/629], Loss: 0.0461\n",
      "Epoch [8/10], Step [100/629], Loss: 0.0820\n",
      "Epoch [8/10], Step [200/629], Loss: 0.1287\n",
      "Epoch [8/10], Step [300/629], Loss: 0.0203\n",
      "Epoch [8/10], Step [400/629], Loss: 0.0785\n",
      "Epoch [8/10], Step [500/629], Loss: 0.0435\n",
      "Epoch [8/10], Step [600/629], Loss: 0.0409\n",
      "Epoch [9/10], Step [100/629], Loss: 0.0529\n",
      "Epoch [9/10], Step [200/629], Loss: 0.0725\n",
      "Epoch [9/10], Step [300/629], Loss: 0.0422\n",
      "Epoch [9/10], Step [400/629], Loss: 0.0656\n",
      "Epoch [9/10], Step [500/629], Loss: 0.0836\n",
      "Epoch [9/10], Step [600/629], Loss: 0.1173\n",
      "Epoch [10/10], Step [100/629], Loss: 0.0093\n",
      "Epoch [10/10], Step [200/629], Loss: 0.0201\n",
      "Epoch [10/10], Step [300/629], Loss: 0.0303\n",
      "Epoch [10/10], Step [400/629], Loss: 0.1032\n",
      "Epoch [10/10], Step [500/629], Loss: 0.0629\n",
      "Epoch [10/10], Step [600/629], Loss: 0.1028\n",
      "Test Accuracy: 96.54%\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 75]          58,875\n",
      "              Tanh-2                   [-1, 75]               0\n",
      "            Linear-3                   [-1, 10]             760\n",
      "================================================================\n",
      "Total params: 59,635\n",
      "Trainable params: 59,635\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.23\n",
      "Estimated Total Size (MB): 0.23\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set the device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 28 * 28  # MNIST image size is 28x28\n",
    "hidden_size = 75\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = MNIST(root='data/', train=True, transform=ToTensor(), download=True)\n",
    "\n",
    "# Split dataset into train and test\n",
    "train_size = int(0.67 * len(train_dataset))\n",
    "test_size = len(train_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(train_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the neural network architecture\n",
    "class TwoLayerNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.activation(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "model = TwoLayerNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Store accuracy in the dictionary with the three-tuple as the key\n",
    "accuracies = {(2, hidden_size, 't'): accuracy}\n",
    "\n",
    "# Print the summary of the model\n",
    "summary(model, (input_size,))\n",
    "\n",
    "# Plot accuracies\n",
    "if not isinstance(labels_graph, list):\n",
    "    labels_graph = labels_graph.tolist()\n",
    "labels_graph.append('2 Layers, 75 Nodes, Tanh')\n",
    "accuracies_values.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb065df6",
   "metadata": {},
   "source": [
    "## Three Layer net with 33 nodes in each and tanh() as activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86dd8852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/629], Loss: 0.7895\n",
      "Epoch [1/10], Step [200/629], Loss: 0.4548\n",
      "Epoch [1/10], Step [300/629], Loss: 0.3303\n",
      "Epoch [1/10], Step [400/629], Loss: 0.3532\n",
      "Epoch [1/10], Step [500/629], Loss: 0.2151\n",
      "Epoch [1/10], Step [600/629], Loss: 0.2154\n",
      "Epoch [2/10], Step [100/629], Loss: 0.3738\n",
      "Epoch [2/10], Step [200/629], Loss: 0.1556\n",
      "Epoch [2/10], Step [300/629], Loss: 0.3089\n",
      "Epoch [2/10], Step [400/629], Loss: 0.3092\n",
      "Epoch [2/10], Step [500/629], Loss: 0.0852\n",
      "Epoch [2/10], Step [600/629], Loss: 0.2480\n",
      "Epoch [3/10], Step [100/629], Loss: 0.2243\n",
      "Epoch [3/10], Step [200/629], Loss: 0.1020\n",
      "Epoch [3/10], Step [300/629], Loss: 0.1236\n",
      "Epoch [3/10], Step [400/629], Loss: 0.1502\n",
      "Epoch [3/10], Step [500/629], Loss: 0.1394\n",
      "Epoch [3/10], Step [600/629], Loss: 0.0413\n",
      "Epoch [4/10], Step [100/629], Loss: 0.2382\n",
      "Epoch [4/10], Step [200/629], Loss: 0.1542\n",
      "Epoch [4/10], Step [300/629], Loss: 0.1061\n",
      "Epoch [4/10], Step [400/629], Loss: 0.0420\n",
      "Epoch [4/10], Step [500/629], Loss: 0.3198\n",
      "Epoch [4/10], Step [600/629], Loss: 0.0880\n",
      "Epoch [5/10], Step [100/629], Loss: 0.0889\n",
      "Epoch [5/10], Step [200/629], Loss: 0.1019\n",
      "Epoch [5/10], Step [300/629], Loss: 0.0524\n",
      "Epoch [5/10], Step [400/629], Loss: 0.2262\n",
      "Epoch [5/10], Step [500/629], Loss: 0.0686\n",
      "Epoch [5/10], Step [600/629], Loss: 0.0461\n",
      "Epoch [6/10], Step [100/629], Loss: 0.0305\n",
      "Epoch [6/10], Step [200/629], Loss: 0.1159\n",
      "Epoch [6/10], Step [300/629], Loss: 0.1962\n",
      "Epoch [6/10], Step [400/629], Loss: 0.0840\n",
      "Epoch [6/10], Step [500/629], Loss: 0.0324\n",
      "Epoch [6/10], Step [600/629], Loss: 0.0334\n",
      "Epoch [7/10], Step [100/629], Loss: 0.0184\n",
      "Epoch [7/10], Step [200/629], Loss: 0.0339\n",
      "Epoch [7/10], Step [300/629], Loss: 0.0259\n",
      "Epoch [7/10], Step [400/629], Loss: 0.0824\n",
      "Epoch [7/10], Step [500/629], Loss: 0.1342\n",
      "Epoch [7/10], Step [600/629], Loss: 0.0462\n",
      "Epoch [8/10], Step [100/629], Loss: 0.0852\n",
      "Epoch [8/10], Step [200/629], Loss: 0.0666\n",
      "Epoch [8/10], Step [300/629], Loss: 0.0590\n",
      "Epoch [8/10], Step [400/629], Loss: 0.0492\n",
      "Epoch [8/10], Step [500/629], Loss: 0.0710\n",
      "Epoch [8/10], Step [600/629], Loss: 0.0522\n",
      "Epoch [9/10], Step [100/629], Loss: 0.0177\n",
      "Epoch [9/10], Step [200/629], Loss: 0.1103\n",
      "Epoch [9/10], Step [300/629], Loss: 0.0545\n",
      "Epoch [9/10], Step [400/629], Loss: 0.0377\n",
      "Epoch [9/10], Step [500/629], Loss: 0.0608\n",
      "Epoch [9/10], Step [600/629], Loss: 0.1551\n",
      "Epoch [10/10], Step [100/629], Loss: 0.0580\n",
      "Epoch [10/10], Step [200/629], Loss: 0.1171\n",
      "Epoch [10/10], Step [300/629], Loss: 0.1295\n",
      "Epoch [10/10], Step [400/629], Loss: 0.0899\n",
      "Epoch [10/10], Step [500/629], Loss: 0.0553\n",
      "Epoch [10/10], Step [600/629], Loss: 0.0892\n",
      "Test Accuracy: 96.10%\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 33]          25,905\n",
      "              Tanh-2                   [-1, 33]               0\n",
      "            Linear-3                   [-1, 33]           1,122\n",
      "              Tanh-4                   [-1, 33]               0\n",
      "            Linear-5                   [-1, 10]             340\n",
      "================================================================\n",
      "Total params: 27,367\n",
      "Trainable params: 27,367\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.10\n",
      "Estimated Total Size (MB): 0.11\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set the device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 28 * 28  # MNIST image size is 28x28\n",
    "hidden_size = 33\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = MNIST(root='data/', train=True, transform=ToTensor(), download=True)\n",
    "\n",
    "# Split dataset into train and test\n",
    "train_size = int(0.67 * len(train_dataset))\n",
    "test_size = len(train_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(train_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the neural network architecture\n",
    "class ThreeLayerNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(ThreeLayerNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.activation(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "model = ThreeLayerNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Store accuracy in the dictionary with the three-tuple as the key\n",
    "accuracies[(3, hidden_size, 't')] = accuracy\n",
    "\n",
    "# Print the summary of the model\n",
    "summary(model, (input_size,))\n",
    "\n",
    "# Plot accuracies\n",
    "if not isinstance(labels_graph, list):\n",
    "    labels_graph = labels_graph.tolist()\n",
    "labels_graph.append('3 Layers, 33 Nodes, Tanh')\n",
    "accuracies_values.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66592f66",
   "metadata": {},
   "source": [
    "## Three Layer net with 50 nodes in each and tanh() as activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9122c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/629], Loss: 0.6101\n",
      "Epoch [1/10], Step [200/629], Loss: 0.4203\n",
      "Epoch [1/10], Step [300/629], Loss: 0.3093\n",
      "Epoch [1/10], Step [400/629], Loss: 0.2440\n",
      "Epoch [1/10], Step [500/629], Loss: 0.3811\n",
      "Epoch [1/10], Step [600/629], Loss: 0.1585\n",
      "Epoch [2/10], Step [100/629], Loss: 0.3184\n",
      "Epoch [2/10], Step [200/629], Loss: 0.0747\n",
      "Epoch [2/10], Step [300/629], Loss: 0.2237\n",
      "Epoch [2/10], Step [400/629], Loss: 0.1321\n",
      "Epoch [2/10], Step [500/629], Loss: 0.0593\n",
      "Epoch [2/10], Step [600/629], Loss: 0.2159\n",
      "Epoch [3/10], Step [100/629], Loss: 0.1720\n",
      "Epoch [3/10], Step [200/629], Loss: 0.1081\n",
      "Epoch [3/10], Step [300/629], Loss: 0.0642\n",
      "Epoch [3/10], Step [400/629], Loss: 0.1091\n",
      "Epoch [3/10], Step [500/629], Loss: 0.0794\n",
      "Epoch [3/10], Step [600/629], Loss: 0.0858\n",
      "Epoch [4/10], Step [100/629], Loss: 0.1504\n",
      "Epoch [4/10], Step [200/629], Loss: 0.1754\n",
      "Epoch [4/10], Step [300/629], Loss: 0.0936\n",
      "Epoch [4/10], Step [400/629], Loss: 0.2575\n",
      "Epoch [4/10], Step [500/629], Loss: 0.1244\n",
      "Epoch [4/10], Step [600/629], Loss: 0.0730\n",
      "Epoch [5/10], Step [100/629], Loss: 0.1702\n",
      "Epoch [5/10], Step [200/629], Loss: 0.0718\n",
      "Epoch [5/10], Step [300/629], Loss: 0.2061\n",
      "Epoch [5/10], Step [400/629], Loss: 0.0370\n",
      "Epoch [5/10], Step [500/629], Loss: 0.2171\n",
      "Epoch [5/10], Step [600/629], Loss: 0.1908\n",
      "Epoch [6/10], Step [100/629], Loss: 0.0499\n",
      "Epoch [6/10], Step [200/629], Loss: 0.1654\n",
      "Epoch [6/10], Step [300/629], Loss: 0.1026\n",
      "Epoch [6/10], Step [400/629], Loss: 0.0658\n",
      "Epoch [6/10], Step [500/629], Loss: 0.0627\n",
      "Epoch [6/10], Step [600/629], Loss: 0.0875\n",
      "Epoch [7/10], Step [100/629], Loss: 0.0213\n",
      "Epoch [7/10], Step [200/629], Loss: 0.0239\n",
      "Epoch [7/10], Step [300/629], Loss: 0.0462\n",
      "Epoch [7/10], Step [400/629], Loss: 0.0365\n",
      "Epoch [7/10], Step [500/629], Loss: 0.0886\n",
      "Epoch [7/10], Step [600/629], Loss: 0.0195\n",
      "Epoch [8/10], Step [100/629], Loss: 0.0230\n",
      "Epoch [8/10], Step [200/629], Loss: 0.0478\n",
      "Epoch [8/10], Step [300/629], Loss: 0.0760\n",
      "Epoch [8/10], Step [400/629], Loss: 0.0454\n",
      "Epoch [8/10], Step [500/629], Loss: 0.0265\n",
      "Epoch [8/10], Step [600/629], Loss: 0.0518\n",
      "Epoch [9/10], Step [100/629], Loss: 0.0921\n",
      "Epoch [9/10], Step [200/629], Loss: 0.0247\n",
      "Epoch [9/10], Step [300/629], Loss: 0.0353\n",
      "Epoch [9/10], Step [400/629], Loss: 0.0170\n",
      "Epoch [9/10], Step [500/629], Loss: 0.0583\n",
      "Epoch [9/10], Step [600/629], Loss: 0.0252\n",
      "Epoch [10/10], Step [100/629], Loss: 0.0234\n",
      "Epoch [10/10], Step [200/629], Loss: 0.0431\n",
      "Epoch [10/10], Step [300/629], Loss: 0.0220\n",
      "Epoch [10/10], Step [400/629], Loss: 0.0238\n",
      "Epoch [10/10], Step [500/629], Loss: 0.1211\n",
      "Epoch [10/10], Step [600/629], Loss: 0.0666\n",
      "Test Accuracy: 96.62%\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 50]          39,250\n",
      "              Tanh-2                   [-1, 50]               0\n",
      "            Linear-3                   [-1, 50]           2,550\n",
      "              Tanh-4                   [-1, 50]               0\n",
      "            Linear-5                   [-1, 10]             510\n",
      "================================================================\n",
      "Total params: 42,310\n",
      "Trainable params: 42,310\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.16\n",
      "Estimated Total Size (MB): 0.17\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set the device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 28 * 28  # MNIST image size is 28x28\n",
    "hidden_size = 50\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = MNIST(root='data/', train=True, transform=ToTensor(), download=True)\n",
    "\n",
    "# Split dataset into train and test\n",
    "train_size = int(0.67 * len(train_dataset))\n",
    "test_size = len(train_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(train_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the neural network architecture\n",
    "class ThreeLayerNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(ThreeLayerNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.activation(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "model = ThreeLayerNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Store accuracy in the dictionary with the three-tuple as the key\n",
    "accuracies[(3, hidden_size, 't')] = accuracy\n",
    "\n",
    "# Print the summary of the model\n",
    "summary(model, (input_size,))\n",
    "\n",
    "# Plot accuracies\n",
    "if not isinstance(labels_graph, list):\n",
    "    labels_graph = labels_graph.tolist()\n",
    "labels_graph.append('3 Layers, 50 Nodes, Tanh')\n",
    "accuracies_values.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7f118b",
   "metadata": {},
   "source": [
    "## Two Layer net with 50 nodes in each layer and sigmoid() activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "190a9898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/629], Loss: 1.3955\n",
      "Epoch [1/10], Step [200/629], Loss: 0.9261\n",
      "Epoch [1/10], Step [300/629], Loss: 0.6345\n",
      "Epoch [1/10], Step [400/629], Loss: 0.5689\n",
      "Epoch [1/10], Step [500/629], Loss: 0.4039\n",
      "Epoch [1/10], Step [600/629], Loss: 0.4382\n",
      "Epoch [2/10], Step [100/629], Loss: 0.4099\n",
      "Epoch [2/10], Step [200/629], Loss: 0.2602\n",
      "Epoch [2/10], Step [300/629], Loss: 0.3783\n",
      "Epoch [2/10], Step [400/629], Loss: 0.2658\n",
      "Epoch [2/10], Step [500/629], Loss: 0.2410\n",
      "Epoch [2/10], Step [600/629], Loss: 0.2531\n",
      "Epoch [3/10], Step [100/629], Loss: 0.2679\n",
      "Epoch [3/10], Step [200/629], Loss: 0.4020\n",
      "Epoch [3/10], Step [300/629], Loss: 0.3813\n",
      "Epoch [3/10], Step [400/629], Loss: 0.2325\n",
      "Epoch [3/10], Step [500/629], Loss: 0.2226\n",
      "Epoch [3/10], Step [600/629], Loss: 0.1944\n",
      "Epoch [4/10], Step [100/629], Loss: 0.3113\n",
      "Epoch [4/10], Step [200/629], Loss: 0.2577\n",
      "Epoch [4/10], Step [300/629], Loss: 0.1907\n",
      "Epoch [4/10], Step [400/629], Loss: 0.3103\n",
      "Epoch [4/10], Step [500/629], Loss: 0.3105\n",
      "Epoch [4/10], Step [600/629], Loss: 0.3014\n",
      "Epoch [5/10], Step [100/629], Loss: 0.1886\n",
      "Epoch [5/10], Step [200/629], Loss: 0.1733\n",
      "Epoch [5/10], Step [300/629], Loss: 0.2176\n",
      "Epoch [5/10], Step [400/629], Loss: 0.2358\n",
      "Epoch [5/10], Step [500/629], Loss: 0.1694\n",
      "Epoch [5/10], Step [600/629], Loss: 0.2040\n",
      "Epoch [6/10], Step [100/629], Loss: 0.1312\n",
      "Epoch [6/10], Step [200/629], Loss: 0.0883\n",
      "Epoch [6/10], Step [300/629], Loss: 0.0602\n",
      "Epoch [6/10], Step [400/629], Loss: 0.2832\n",
      "Epoch [6/10], Step [500/629], Loss: 0.3229\n",
      "Epoch [6/10], Step [600/629], Loss: 0.1817\n",
      "Epoch [7/10], Step [100/629], Loss: 0.1120\n",
      "Epoch [7/10], Step [200/629], Loss: 0.1733\n",
      "Epoch [7/10], Step [300/629], Loss: 0.1980\n",
      "Epoch [7/10], Step [400/629], Loss: 0.2074\n",
      "Epoch [7/10], Step [500/629], Loss: 0.1280\n",
      "Epoch [7/10], Step [600/629], Loss: 0.1457\n",
      "Epoch [8/10], Step [100/629], Loss: 0.1033\n",
      "Epoch [8/10], Step [200/629], Loss: 0.0752\n",
      "Epoch [8/10], Step [300/629], Loss: 0.0871\n",
      "Epoch [8/10], Step [400/629], Loss: 0.1206\n",
      "Epoch [8/10], Step [500/629], Loss: 0.1537\n",
      "Epoch [8/10], Step [600/629], Loss: 0.1679\n",
      "Epoch [9/10], Step [100/629], Loss: 0.3974\n",
      "Epoch [9/10], Step [200/629], Loss: 0.1695\n",
      "Epoch [9/10], Step [300/629], Loss: 0.2042\n",
      "Epoch [9/10], Step [400/629], Loss: 0.1206\n",
      "Epoch [9/10], Step [500/629], Loss: 0.0944\n",
      "Epoch [9/10], Step [600/629], Loss: 0.1000\n",
      "Epoch [10/10], Step [100/629], Loss: 0.1134\n",
      "Epoch [10/10], Step [200/629], Loss: 0.0630\n",
      "Epoch [10/10], Step [300/629], Loss: 0.0310\n",
      "Epoch [10/10], Step [400/629], Loss: 0.1047\n",
      "Epoch [10/10], Step [500/629], Loss: 0.1375\n",
      "Epoch [10/10], Step [600/629], Loss: 0.0650\n",
      "Test Accuracy: 95.53%\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 50]          39,250\n",
      "           Sigmoid-2                   [-1, 50]               0\n",
      "            Linear-3                   [-1, 10]             510\n",
      "================================================================\n",
      "Total params: 39,760\n",
      "Trainable params: 39,760\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.15\n",
      "Estimated Total Size (MB): 0.16\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set the device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 28 * 28  # MNIST image size is 28x28\n",
    "hidden_size = 50\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = MNIST(root='data/', train=True, transform=ToTensor(), download=True)\n",
    "\n",
    "# Split dataset into train and test\n",
    "train_size = int(0.67 * len(train_dataset))\n",
    "test_size = len(train_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(train_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the neural network architecture\n",
    "class TwoLayerNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.activation(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "model = TwoLayerNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Store accuracy in the dictionary with the three-tuple as the key\n",
    "accuracies[(2, hidden_size, 's')] = accuracy\n",
    "\n",
    "# Print the summary of the model\n",
    "summary(model, (input_size,))\n",
    "\n",
    "# Plot accuracies\n",
    "if not isinstance(labels_graph, list):\n",
    "    labels_graph = labels_graph.tolist()\n",
    "labels_graph.append('2 Layers, 50 Nodes, Sigmoid')\n",
    "accuracies_values.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2793e50",
   "metadata": {},
   "source": [
    "## Two Layer net with 60 nodes in each layer and sigmoid() activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1545a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/629], Loss: 1.3745\n",
      "Epoch [1/10], Step [200/629], Loss: 0.8219\n",
      "Epoch [1/10], Step [300/629], Loss: 0.6699\n",
      "Epoch [1/10], Step [400/629], Loss: 0.4602\n",
      "Epoch [1/10], Step [500/629], Loss: 0.4003\n",
      "Epoch [1/10], Step [600/629], Loss: 0.3600\n",
      "Epoch [2/10], Step [100/629], Loss: 0.4422\n",
      "Epoch [2/10], Step [200/629], Loss: 0.3968\n",
      "Epoch [2/10], Step [300/629], Loss: 0.2043\n",
      "Epoch [2/10], Step [400/629], Loss: 0.2258\n",
      "Epoch [2/10], Step [500/629], Loss: 0.2811\n",
      "Epoch [2/10], Step [600/629], Loss: 0.2446\n",
      "Epoch [3/10], Step [100/629], Loss: 0.2218\n",
      "Epoch [3/10], Step [200/629], Loss: 0.1174\n",
      "Epoch [3/10], Step [300/629], Loss: 0.2859\n",
      "Epoch [3/10], Step [400/629], Loss: 0.2152\n",
      "Epoch [3/10], Step [500/629], Loss: 0.2721\n",
      "Epoch [3/10], Step [600/629], Loss: 0.1750\n",
      "Epoch [4/10], Step [100/629], Loss: 0.2089\n",
      "Epoch [4/10], Step [200/629], Loss: 0.1557\n",
      "Epoch [4/10], Step [300/629], Loss: 0.2371\n",
      "Epoch [4/10], Step [400/629], Loss: 0.1863\n",
      "Epoch [4/10], Step [500/629], Loss: 0.1641\n",
      "Epoch [4/10], Step [600/629], Loss: 0.2845\n",
      "Epoch [5/10], Step [100/629], Loss: 0.1613\n",
      "Epoch [5/10], Step [200/629], Loss: 0.0851\n",
      "Epoch [5/10], Step [300/629], Loss: 0.1917\n",
      "Epoch [5/10], Step [400/629], Loss: 0.2011\n",
      "Epoch [5/10], Step [500/629], Loss: 0.1656\n",
      "Epoch [5/10], Step [600/629], Loss: 0.1656\n",
      "Epoch [6/10], Step [100/629], Loss: 0.1139\n",
      "Epoch [6/10], Step [200/629], Loss: 0.2455\n",
      "Epoch [6/10], Step [300/629], Loss: 0.1129\n",
      "Epoch [6/10], Step [400/629], Loss: 0.2593\n",
      "Epoch [6/10], Step [500/629], Loss: 0.2840\n",
      "Epoch [6/10], Step [600/629], Loss: 0.1105\n",
      "Epoch [7/10], Step [100/629], Loss: 0.3132\n",
      "Epoch [7/10], Step [200/629], Loss: 0.1346\n",
      "Epoch [7/10], Step [300/629], Loss: 0.2129\n",
      "Epoch [7/10], Step [400/629], Loss: 0.1424\n",
      "Epoch [7/10], Step [500/629], Loss: 0.1440\n",
      "Epoch [7/10], Step [600/629], Loss: 0.1623\n",
      "Epoch [8/10], Step [100/629], Loss: 0.1324\n",
      "Epoch [8/10], Step [200/629], Loss: 0.1075\n",
      "Epoch [8/10], Step [300/629], Loss: 0.1445\n",
      "Epoch [8/10], Step [400/629], Loss: 0.0608\n",
      "Epoch [8/10], Step [500/629], Loss: 0.1245\n",
      "Epoch [8/10], Step [600/629], Loss: 0.0541\n",
      "Epoch [9/10], Step [100/629], Loss: 0.1046\n",
      "Epoch [9/10], Step [200/629], Loss: 0.0921\n",
      "Epoch [9/10], Step [300/629], Loss: 0.2500\n",
      "Epoch [9/10], Step [400/629], Loss: 0.1024\n",
      "Epoch [9/10], Step [500/629], Loss: 0.0386\n",
      "Epoch [9/10], Step [600/629], Loss: 0.0626\n",
      "Epoch [10/10], Step [100/629], Loss: 0.1554\n",
      "Epoch [10/10], Step [200/629], Loss: 0.1857\n",
      "Epoch [10/10], Step [300/629], Loss: 0.1529\n",
      "Epoch [10/10], Step [400/629], Loss: 0.0578\n",
      "Epoch [10/10], Step [500/629], Loss: 0.0914\n",
      "Epoch [10/10], Step [600/629], Loss: 0.0562\n",
      "Test Accuracy: 96.01%\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 60]          47,100\n",
      "           Sigmoid-2                   [-1, 60]               0\n",
      "            Linear-3                   [-1, 10]             610\n",
      "================================================================\n",
      "Total params: 47,710\n",
      "Trainable params: 47,710\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.18\n",
      "Estimated Total Size (MB): 0.19\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set the device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 28 * 28  # MNIST image size is 28x28\n",
    "hidden_size = 60\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = MNIST(root='data/', train=True, transform=ToTensor(), download=True)\n",
    "\n",
    "# Split dataset into train and test\n",
    "train_size = int(0.67 * len(train_dataset))\n",
    "test_size = len(train_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(train_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the neural network architecture\n",
    "class TwoLayerNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.activation(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "model = TwoLayerNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Store accuracy in the dictionary with the three-tuple as the key\n",
    "accuracies[(2, hidden_size, 's')] = accuracy\n",
    "\n",
    "# Print the summary of the model\n",
    "summary(model, (input_size,))\n",
    "\n",
    "# Plot accuracies\n",
    "if not isinstance(labels_graph, list):\n",
    "    labels_graph = labels_graph.tolist()\n",
    "labels_graph.append('2 Layers, 60 Nodes, Sigmoid')\n",
    "accuracies_values.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bc85f3",
   "metadata": {},
   "source": [
    "## Two Layer net with 75 nodes in each layer and sigmoid() activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "403533e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/629], Loss: 1.2296\n",
      "Epoch [1/10], Step [200/629], Loss: 0.6287\n",
      "Epoch [1/10], Step [300/629], Loss: 0.5213\n",
      "Epoch [1/10], Step [400/629], Loss: 0.5172\n",
      "Epoch [1/10], Step [500/629], Loss: 0.4767\n",
      "Epoch [1/10], Step [600/629], Loss: 0.3656\n",
      "Epoch [2/10], Step [100/629], Loss: 0.1805\n",
      "Epoch [2/10], Step [200/629], Loss: 0.3886\n",
      "Epoch [2/10], Step [300/629], Loss: 0.4106\n",
      "Epoch [2/10], Step [400/629], Loss: 0.2573\n",
      "Epoch [2/10], Step [500/629], Loss: 0.1913\n",
      "Epoch [2/10], Step [600/629], Loss: 0.3623\n",
      "Epoch [3/10], Step [100/629], Loss: 0.5257\n",
      "Epoch [3/10], Step [200/629], Loss: 0.1462\n",
      "Epoch [3/10], Step [300/629], Loss: 0.1842\n",
      "Epoch [3/10], Step [400/629], Loss: 0.2300\n",
      "Epoch [3/10], Step [500/629], Loss: 0.1082\n",
      "Epoch [3/10], Step [600/629], Loss: 0.3070\n",
      "Epoch [4/10], Step [100/629], Loss: 0.0763\n",
      "Epoch [4/10], Step [200/629], Loss: 0.1172\n",
      "Epoch [4/10], Step [300/629], Loss: 0.2757\n",
      "Epoch [4/10], Step [400/629], Loss: 0.2237\n",
      "Epoch [4/10], Step [500/629], Loss: 0.1421\n",
      "Epoch [4/10], Step [600/629], Loss: 0.1600\n",
      "Epoch [5/10], Step [100/629], Loss: 0.2622\n",
      "Epoch [5/10], Step [200/629], Loss: 0.1479\n",
      "Epoch [5/10], Step [300/629], Loss: 0.2378\n",
      "Epoch [5/10], Step [400/629], Loss: 0.2435\n",
      "Epoch [5/10], Step [500/629], Loss: 0.2044\n",
      "Epoch [5/10], Step [600/629], Loss: 0.2499\n",
      "Epoch [6/10], Step [100/629], Loss: 0.1547\n",
      "Epoch [6/10], Step [200/629], Loss: 0.2014\n",
      "Epoch [6/10], Step [300/629], Loss: 0.0655\n",
      "Epoch [6/10], Step [400/629], Loss: 0.1293\n",
      "Epoch [6/10], Step [500/629], Loss: 0.1644\n",
      "Epoch [6/10], Step [600/629], Loss: 0.1412\n",
      "Epoch [7/10], Step [100/629], Loss: 0.2259\n",
      "Epoch [7/10], Step [200/629], Loss: 0.1058\n",
      "Epoch [7/10], Step [300/629], Loss: 0.1366\n",
      "Epoch [7/10], Step [400/629], Loss: 0.1520\n",
      "Epoch [7/10], Step [500/629], Loss: 0.0473\n",
      "Epoch [7/10], Step [600/629], Loss: 0.1661\n",
      "Epoch [8/10], Step [100/629], Loss: 0.0942\n",
      "Epoch [8/10], Step [200/629], Loss: 0.1094\n",
      "Epoch [8/10], Step [300/629], Loss: 0.0627\n",
      "Epoch [8/10], Step [400/629], Loss: 0.0812\n",
      "Epoch [8/10], Step [500/629], Loss: 0.0942\n",
      "Epoch [8/10], Step [600/629], Loss: 0.2869\n",
      "Epoch [9/10], Step [100/629], Loss: 0.1675\n",
      "Epoch [9/10], Step [200/629], Loss: 0.0740\n",
      "Epoch [9/10], Step [300/629], Loss: 0.0717\n",
      "Epoch [9/10], Step [400/629], Loss: 0.0412\n",
      "Epoch [9/10], Step [500/629], Loss: 0.1889\n",
      "Epoch [9/10], Step [600/629], Loss: 0.1211\n",
      "Epoch [10/10], Step [100/629], Loss: 0.1683\n",
      "Epoch [10/10], Step [200/629], Loss: 0.1413\n",
      "Epoch [10/10], Step [300/629], Loss: 0.1234\n",
      "Epoch [10/10], Step [400/629], Loss: 0.1632\n",
      "Epoch [10/10], Step [500/629], Loss: 0.0707\n",
      "Epoch [10/10], Step [600/629], Loss: 0.0805\n",
      "Test Accuracy: 95.91%\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 75]          58,875\n",
      "           Sigmoid-2                   [-1, 75]               0\n",
      "            Linear-3                   [-1, 10]             760\n",
      "================================================================\n",
      "Total params: 59,635\n",
      "Trainable params: 59,635\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.23\n",
      "Estimated Total Size (MB): 0.23\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set the device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 28 * 28  # MNIST image size is 28x28\n",
    "hidden_size = 75\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = MNIST(root='data/', train=True, transform=ToTensor(), download=True)\n",
    "\n",
    "# Split dataset into train and test\n",
    "train_size = int(0.67 * len(train_dataset))\n",
    "test_size = len(train_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(train_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the neural network architecture\n",
    "class TwoLayerNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.activation(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "model = TwoLayerNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Store accuracy in the dictionary with the three-tuple as the key\n",
    "accuracies[(2, hidden_size, 's')] = accuracy\n",
    "\n",
    "# Print the summary of the model\n",
    "summary(model, (input_size,))\n",
    "\n",
    "# Plot accuracies\n",
    "if not isinstance(labels_graph, list):\n",
    "    labels_graph = labels_graph.tolist()\n",
    "labels_graph.append('2 Layers, 75 Nodes, Sigmoid')\n",
    "accuracies_values.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320d44c7",
   "metadata": {},
   "source": [
    "## Three Layer net with 33 nodes in each and sigmoid() as activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fba0a411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/629], Loss: 2.1037\n",
      "Epoch [1/10], Step [200/629], Loss: 1.6698\n",
      "Epoch [1/10], Step [300/629], Loss: 1.2545\n",
      "Epoch [1/10], Step [400/629], Loss: 0.9429\n",
      "Epoch [1/10], Step [500/629], Loss: 0.7868\n",
      "Epoch [1/10], Step [600/629], Loss: 0.6321\n",
      "Epoch [2/10], Step [100/629], Loss: 0.5251\n",
      "Epoch [2/10], Step [200/629], Loss: 0.5472\n",
      "Epoch [2/10], Step [300/629], Loss: 0.3867\n",
      "Epoch [2/10], Step [400/629], Loss: 0.3853\n",
      "Epoch [2/10], Step [500/629], Loss: 0.4431\n",
      "Epoch [2/10], Step [600/629], Loss: 0.4714\n",
      "Epoch [3/10], Step [100/629], Loss: 0.3705\n",
      "Epoch [3/10], Step [200/629], Loss: 0.3208\n",
      "Epoch [3/10], Step [300/629], Loss: 0.2523\n",
      "Epoch [3/10], Step [400/629], Loss: 0.3642\n",
      "Epoch [3/10], Step [500/629], Loss: 0.3597\n",
      "Epoch [3/10], Step [600/629], Loss: 0.2340\n",
      "Epoch [4/10], Step [100/629], Loss: 0.3400\n",
      "Epoch [4/10], Step [200/629], Loss: 0.3007\n",
      "Epoch [4/10], Step [300/629], Loss: 0.2686\n",
      "Epoch [4/10], Step [400/629], Loss: 0.1717\n",
      "Epoch [4/10], Step [500/629], Loss: 0.2279\n",
      "Epoch [4/10], Step [600/629], Loss: 0.1996\n",
      "Epoch [5/10], Step [100/629], Loss: 0.1841\n",
      "Epoch [5/10], Step [200/629], Loss: 0.0953\n",
      "Epoch [5/10], Step [300/629], Loss: 0.0921\n",
      "Epoch [5/10], Step [400/629], Loss: 0.2936\n",
      "Epoch [5/10], Step [500/629], Loss: 0.1780\n",
      "Epoch [5/10], Step [600/629], Loss: 0.1493\n",
      "Epoch [6/10], Step [100/629], Loss: 0.2045\n",
      "Epoch [6/10], Step [200/629], Loss: 0.1979\n",
      "Epoch [6/10], Step [300/629], Loss: 0.2011\n",
      "Epoch [6/10], Step [400/629], Loss: 0.3615\n",
      "Epoch [6/10], Step [500/629], Loss: 0.0871\n",
      "Epoch [6/10], Step [600/629], Loss: 0.2637\n",
      "Epoch [7/10], Step [100/629], Loss: 0.1275\n",
      "Epoch [7/10], Step [200/629], Loss: 0.2164\n",
      "Epoch [7/10], Step [300/629], Loss: 0.1926\n",
      "Epoch [7/10], Step [400/629], Loss: 0.1083\n",
      "Epoch [7/10], Step [500/629], Loss: 0.2566\n",
      "Epoch [7/10], Step [600/629], Loss: 0.1206\n",
      "Epoch [8/10], Step [100/629], Loss: 0.2388\n",
      "Epoch [8/10], Step [200/629], Loss: 0.2075\n",
      "Epoch [8/10], Step [300/629], Loss: 0.2014\n",
      "Epoch [8/10], Step [400/629], Loss: 0.1360\n",
      "Epoch [8/10], Step [500/629], Loss: 0.1773\n",
      "Epoch [8/10], Step [600/629], Loss: 0.2694\n",
      "Epoch [9/10], Step [100/629], Loss: 0.2163\n",
      "Epoch [9/10], Step [200/629], Loss: 0.0869\n",
      "Epoch [9/10], Step [300/629], Loss: 0.0654\n",
      "Epoch [9/10], Step [400/629], Loss: 0.1563\n",
      "Epoch [9/10], Step [500/629], Loss: 0.0552\n",
      "Epoch [9/10], Step [600/629], Loss: 0.2435\n",
      "Epoch [10/10], Step [100/629], Loss: 0.1355\n",
      "Epoch [10/10], Step [200/629], Loss: 0.0850\n",
      "Epoch [10/10], Step [300/629], Loss: 0.2222\n",
      "Epoch [10/10], Step [400/629], Loss: 0.1755\n",
      "Epoch [10/10], Step [500/629], Loss: 0.1560\n",
      "Epoch [10/10], Step [600/629], Loss: 0.1840\n",
      "Test Accuracy: 95.17%\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 33]          25,905\n",
      "           Sigmoid-2                   [-1, 33]               0\n",
      "            Linear-3                   [-1, 33]           1,122\n",
      "           Sigmoid-4                   [-1, 33]               0\n",
      "            Linear-5                   [-1, 10]             340\n",
      "================================================================\n",
      "Total params: 27,367\n",
      "Trainable params: 27,367\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.10\n",
      "Estimated Total Size (MB): 0.11\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set the device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 28 * 28  # MNIST image size is 28x28\n",
    "hidden_size = 33\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = MNIST(root='data/', train=True, transform=ToTensor(), download=True)\n",
    "\n",
    "# Split dataset into train and test\n",
    "train_size = int(0.67 * len(train_dataset))\n",
    "test_size = len(train_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(train_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the neural network architecture\n",
    "class ThreeLayerNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(ThreeLayerNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.activation(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "model = ThreeLayerNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Store accuracy in the dictionary with the three-tuple as the key\n",
    "accuracies[(3, hidden_size, 's')] = accuracy\n",
    "\n",
    "# Print the summary of the model\n",
    "summary(model, (input_size,))\n",
    "\n",
    "# Plot accuracies\n",
    "if not isinstance(labels_graph, list):\n",
    "    labels_graph = labels_graph.tolist()\n",
    "labels_graph.append('3 Layers, 33 Nodes, Sigmoid')\n",
    "accuracies_values.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8183b1",
   "metadata": {},
   "source": [
    "## Three Layer net with 50 nodes in each and sigmoid() as activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c68ef806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/629], Loss: 2.0228\n",
      "Epoch [1/10], Step [200/629], Loss: 1.3376\n",
      "Epoch [1/10], Step [300/629], Loss: 1.0615\n",
      "Epoch [1/10], Step [400/629], Loss: 0.8113\n",
      "Epoch [1/10], Step [500/629], Loss: 0.5630\n",
      "Epoch [1/10], Step [600/629], Loss: 0.4606\n",
      "Epoch [2/10], Step [100/629], Loss: 0.3794\n",
      "Epoch [2/10], Step [200/629], Loss: 0.4634\n",
      "Epoch [2/10], Step [300/629], Loss: 0.3592\n",
      "Epoch [2/10], Step [400/629], Loss: 0.5013\n",
      "Epoch [2/10], Step [500/629], Loss: 0.4684\n",
      "Epoch [2/10], Step [600/629], Loss: 0.2504\n",
      "Epoch [3/10], Step [100/629], Loss: 0.2825\n",
      "Epoch [3/10], Step [200/629], Loss: 0.1838\n",
      "Epoch [3/10], Step [300/629], Loss: 0.3947\n",
      "Epoch [3/10], Step [400/629], Loss: 0.3070\n",
      "Epoch [3/10], Step [500/629], Loss: 0.2591\n",
      "Epoch [3/10], Step [600/629], Loss: 0.1206\n",
      "Epoch [4/10], Step [100/629], Loss: 0.1950\n",
      "Epoch [4/10], Step [200/629], Loss: 0.2714\n",
      "Epoch [4/10], Step [300/629], Loss: 0.2831\n",
      "Epoch [4/10], Step [400/629], Loss: 0.2139\n",
      "Epoch [4/10], Step [500/629], Loss: 0.3847\n",
      "Epoch [4/10], Step [600/629], Loss: 0.1766\n",
      "Epoch [5/10], Step [100/629], Loss: 0.2689\n",
      "Epoch [5/10], Step [200/629], Loss: 0.2145\n",
      "Epoch [5/10], Step [300/629], Loss: 0.1272\n",
      "Epoch [5/10], Step [400/629], Loss: 0.1523\n",
      "Epoch [5/10], Step [500/629], Loss: 0.1392\n",
      "Epoch [5/10], Step [600/629], Loss: 0.1950\n",
      "Epoch [6/10], Step [100/629], Loss: 0.1685\n",
      "Epoch [6/10], Step [200/629], Loss: 0.0883\n",
      "Epoch [6/10], Step [300/629], Loss: 0.1418\n",
      "Epoch [6/10], Step [400/629], Loss: 0.4052\n",
      "Epoch [6/10], Step [500/629], Loss: 0.1467\n",
      "Epoch [6/10], Step [600/629], Loss: 0.1070\n",
      "Epoch [7/10], Step [100/629], Loss: 0.0807\n",
      "Epoch [7/10], Step [200/629], Loss: 0.1246\n",
      "Epoch [7/10], Step [300/629], Loss: 0.0927\n",
      "Epoch [7/10], Step [400/629], Loss: 0.1400\n",
      "Epoch [7/10], Step [500/629], Loss: 0.1129\n",
      "Epoch [7/10], Step [600/629], Loss: 0.0980\n",
      "Epoch [8/10], Step [100/629], Loss: 0.1895\n",
      "Epoch [8/10], Step [200/629], Loss: 0.2292\n",
      "Epoch [8/10], Step [300/629], Loss: 0.1215\n",
      "Epoch [8/10], Step [400/629], Loss: 0.0926\n",
      "Epoch [8/10], Step [500/629], Loss: 0.0197\n",
      "Epoch [8/10], Step [600/629], Loss: 0.4237\n",
      "Epoch [9/10], Step [100/629], Loss: 0.0806\n",
      "Epoch [9/10], Step [200/629], Loss: 0.1188\n",
      "Epoch [9/10], Step [300/629], Loss: 0.0998\n",
      "Epoch [9/10], Step [400/629], Loss: 0.1047\n",
      "Epoch [9/10], Step [500/629], Loss: 0.0417\n",
      "Epoch [9/10], Step [600/629], Loss: 0.2050\n",
      "Epoch [10/10], Step [100/629], Loss: 0.0373\n",
      "Epoch [10/10], Step [200/629], Loss: 0.0484\n",
      "Epoch [10/10], Step [300/629], Loss: 0.0858\n",
      "Epoch [10/10], Step [400/629], Loss: 0.0495\n",
      "Epoch [10/10], Step [500/629], Loss: 0.0614\n",
      "Epoch [10/10], Step [600/629], Loss: 0.1210\n",
      "Test Accuracy: 95.86%\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 50]          39,250\n",
      "           Sigmoid-2                   [-1, 50]               0\n",
      "            Linear-3                   [-1, 50]           2,550\n",
      "           Sigmoid-4                   [-1, 50]               0\n",
      "            Linear-5                   [-1, 10]             510\n",
      "================================================================\n",
      "Total params: 42,310\n",
      "Trainable params: 42,310\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.16\n",
      "Estimated Total Size (MB): 0.17\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set the device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 28 * 28  # MNIST image size is 28x28\n",
    "hidden_size = 50\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = MNIST(root='data/', train=True, transform=ToTensor(), download=True)\n",
    "\n",
    "# Split dataset into train and test\n",
    "train_size = int(0.67 * len(train_dataset))\n",
    "test_size = len(train_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(train_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the neural network architecture\n",
    "class ThreeLayerNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(ThreeLayerNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.activation(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "model = ThreeLayerNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Store accuracy in the dictionary with the three-tuple as the key\n",
    "accuracies[(3, hidden_size, 't')] = accuracy\n",
    "\n",
    "# Print the summary of the model\n",
    "summary(model, (input_size,))\n",
    "\n",
    "# Plot accuracies\n",
    "if not isinstance(labels_graph, list):\n",
    "    labels_graph = labels_graph.tolist()\n",
    "labels_graph.append('3 Layers, 50 Nodes, Sigmoid')\n",
    "accuracies_values.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f80cd8",
   "metadata": {},
   "source": [
    "## Two Layer net with 50 nodes in each layer and ReLU() activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d682767a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/629], Loss: 0.7438\n",
      "Epoch [1/10], Step [200/629], Loss: 0.3192\n",
      "Epoch [1/10], Step [300/629], Loss: 0.2898\n",
      "Epoch [1/10], Step [400/629], Loss: 0.2938\n",
      "Epoch [1/10], Step [500/629], Loss: 0.1739\n",
      "Epoch [1/10], Step [600/629], Loss: 0.2954\n",
      "Epoch [2/10], Step [100/629], Loss: 0.4180\n",
      "Epoch [2/10], Step [200/629], Loss: 0.3174\n",
      "Epoch [2/10], Step [300/629], Loss: 0.2713\n",
      "Epoch [2/10], Step [400/629], Loss: 0.2393\n",
      "Epoch [2/10], Step [500/629], Loss: 0.2427\n",
      "Epoch [2/10], Step [600/629], Loss: 0.2303\n",
      "Epoch [3/10], Step [100/629], Loss: 0.3429\n",
      "Epoch [3/10], Step [200/629], Loss: 0.3187\n",
      "Epoch [3/10], Step [300/629], Loss: 0.3169\n",
      "Epoch [3/10], Step [400/629], Loss: 0.1225\n",
      "Epoch [3/10], Step [500/629], Loss: 0.0705\n",
      "Epoch [3/10], Step [600/629], Loss: 0.2945\n",
      "Epoch [4/10], Step [100/629], Loss: 0.0481\n",
      "Epoch [4/10], Step [200/629], Loss: 0.2012\n",
      "Epoch [4/10], Step [300/629], Loss: 0.1529\n",
      "Epoch [4/10], Step [400/629], Loss: 0.0910\n",
      "Epoch [4/10], Step [500/629], Loss: 0.1358\n",
      "Epoch [4/10], Step [600/629], Loss: 0.1833\n",
      "Epoch [5/10], Step [100/629], Loss: 0.1714\n",
      "Epoch [5/10], Step [200/629], Loss: 0.0955\n",
      "Epoch [5/10], Step [300/629], Loss: 0.1802\n",
      "Epoch [5/10], Step [400/629], Loss: 0.2769\n",
      "Epoch [5/10], Step [500/629], Loss: 0.1005\n",
      "Epoch [5/10], Step [600/629], Loss: 0.0855\n",
      "Epoch [6/10], Step [100/629], Loss: 0.0833\n",
      "Epoch [6/10], Step [200/629], Loss: 0.1890\n",
      "Epoch [6/10], Step [300/629], Loss: 0.0838\n",
      "Epoch [6/10], Step [400/629], Loss: 0.0600\n",
      "Epoch [6/10], Step [500/629], Loss: 0.1074\n",
      "Epoch [6/10], Step [600/629], Loss: 0.1005\n",
      "Epoch [7/10], Step [100/629], Loss: 0.0324\n",
      "Epoch [7/10], Step [200/629], Loss: 0.0592\n",
      "Epoch [7/10], Step [300/629], Loss: 0.0672\n",
      "Epoch [7/10], Step [400/629], Loss: 0.0754\n",
      "Epoch [7/10], Step [500/629], Loss: 0.2061\n",
      "Epoch [7/10], Step [600/629], Loss: 0.0337\n",
      "Epoch [8/10], Step [100/629], Loss: 0.0783\n",
      "Epoch [8/10], Step [200/629], Loss: 0.0410\n",
      "Epoch [8/10], Step [300/629], Loss: 0.0840\n",
      "Epoch [8/10], Step [400/629], Loss: 0.0603\n",
      "Epoch [8/10], Step [500/629], Loss: 0.1050\n",
      "Epoch [8/10], Step [600/629], Loss: 0.0694\n",
      "Epoch [9/10], Step [100/629], Loss: 0.0477\n",
      "Epoch [9/10], Step [200/629], Loss: 0.0507\n",
      "Epoch [9/10], Step [300/629], Loss: 0.1438\n",
      "Epoch [9/10], Step [400/629], Loss: 0.0205\n",
      "Epoch [9/10], Step [500/629], Loss: 0.1249\n",
      "Epoch [9/10], Step [600/629], Loss: 0.0233\n",
      "Epoch [10/10], Step [100/629], Loss: 0.0274\n",
      "Epoch [10/10], Step [200/629], Loss: 0.2127\n",
      "Epoch [10/10], Step [300/629], Loss: 0.0399\n",
      "Epoch [10/10], Step [400/629], Loss: 0.0677\n",
      "Epoch [10/10], Step [500/629], Loss: 0.0350\n",
      "Epoch [10/10], Step [600/629], Loss: 0.0154\n",
      "Test Accuracy: 96.46%\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 50]          39,250\n",
      "              ReLU-2                   [-1, 50]               0\n",
      "            Linear-3                   [-1, 10]             510\n",
      "================================================================\n",
      "Total params: 39,760\n",
      "Trainable params: 39,760\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.15\n",
      "Estimated Total Size (MB): 0.16\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set the device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 28 * 28  # MNIST image size is 28x28\n",
    "hidden_size = 50\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = MNIST(root='data/', train=True, transform=ToTensor(), download=True)\n",
    "\n",
    "# Split dataset into train and test\n",
    "train_size = int(0.67 * len(train_dataset))\n",
    "test_size = len(train_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(train_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the neural network architecture\n",
    "class TwoLayerNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.activation = nn.ReLU()  \n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.activation(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "model = TwoLayerNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Store accuracy in the dictionary with the three-tuple as the key\n",
    "accuracies[(2, hidden_size, 'r')] = accuracy\n",
    "\n",
    "# Print the summary of the model\n",
    "summary(model, (input_size,))\n",
    "\n",
    "# Plot accuracies\n",
    "if not isinstance(labels_graph, list):\n",
    "    labels_graph = labels_graph.tolist()\n",
    "labels_graph.append('2 Layers, 50 Nodes, ReLU')\n",
    "accuracies_values.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bfc6f8",
   "metadata": {},
   "source": [
    "## Two Layer net with 60 nodes in each layer and ReLU() activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "009aaa7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/629], Loss: 0.4781\n",
      "Epoch [1/10], Step [200/629], Loss: 0.2553\n",
      "Epoch [1/10], Step [300/629], Loss: 0.3585\n",
      "Epoch [1/10], Step [400/629], Loss: 0.4366\n",
      "Epoch [1/10], Step [500/629], Loss: 0.2260\n",
      "Epoch [1/10], Step [600/629], Loss: 0.2275\n",
      "Epoch [2/10], Step [100/629], Loss: 0.3109\n",
      "Epoch [2/10], Step [200/629], Loss: 0.5778\n",
      "Epoch [2/10], Step [300/629], Loss: 0.2331\n",
      "Epoch [2/10], Step [400/629], Loss: 0.2928\n",
      "Epoch [2/10], Step [500/629], Loss: 0.1786\n",
      "Epoch [2/10], Step [600/629], Loss: 0.2290\n",
      "Epoch [3/10], Step [100/629], Loss: 0.1075\n",
      "Epoch [3/10], Step [200/629], Loss: 0.1546\n",
      "Epoch [3/10], Step [300/629], Loss: 0.2216\n",
      "Epoch [3/10], Step [400/629], Loss: 0.1809\n",
      "Epoch [3/10], Step [500/629], Loss: 0.0579\n",
      "Epoch [3/10], Step [600/629], Loss: 0.1275\n",
      "Epoch [4/10], Step [100/629], Loss: 0.1358\n",
      "Epoch [4/10], Step [200/629], Loss: 0.2533\n",
      "Epoch [4/10], Step [300/629], Loss: 0.2480\n",
      "Epoch [4/10], Step [400/629], Loss: 0.0881\n",
      "Epoch [4/10], Step [500/629], Loss: 0.0881\n",
      "Epoch [4/10], Step [600/629], Loss: 0.1964\n",
      "Epoch [5/10], Step [100/629], Loss: 0.0446\n",
      "Epoch [5/10], Step [200/629], Loss: 0.1336\n",
      "Epoch [5/10], Step [300/629], Loss: 0.2166\n",
      "Epoch [5/10], Step [400/629], Loss: 0.0706\n",
      "Epoch [5/10], Step [500/629], Loss: 0.1147\n",
      "Epoch [5/10], Step [600/629], Loss: 0.1060\n",
      "Epoch [6/10], Step [100/629], Loss: 0.0614\n",
      "Epoch [6/10], Step [200/629], Loss: 0.1125\n",
      "Epoch [6/10], Step [300/629], Loss: 0.0166\n",
      "Epoch [6/10], Step [400/629], Loss: 0.1920\n",
      "Epoch [6/10], Step [500/629], Loss: 0.1786\n",
      "Epoch [6/10], Step [600/629], Loss: 0.2056\n",
      "Epoch [7/10], Step [100/629], Loss: 0.0624\n",
      "Epoch [7/10], Step [200/629], Loss: 0.0701\n",
      "Epoch [7/10], Step [300/629], Loss: 0.1378\n",
      "Epoch [7/10], Step [400/629], Loss: 0.0407\n",
      "Epoch [7/10], Step [500/629], Loss: 0.2174\n",
      "Epoch [7/10], Step [600/629], Loss: 0.0932\n",
      "Epoch [8/10], Step [100/629], Loss: 0.0584\n",
      "Epoch [8/10], Step [200/629], Loss: 0.0478\n",
      "Epoch [8/10], Step [300/629], Loss: 0.0474\n",
      "Epoch [8/10], Step [400/629], Loss: 0.0894\n",
      "Epoch [8/10], Step [500/629], Loss: 0.0611\n",
      "Epoch [8/10], Step [600/629], Loss: 0.1293\n",
      "Epoch [9/10], Step [100/629], Loss: 0.0661\n",
      "Epoch [9/10], Step [200/629], Loss: 0.0634\n",
      "Epoch [9/10], Step [300/629], Loss: 0.1066\n",
      "Epoch [9/10], Step [400/629], Loss: 0.0275\n",
      "Epoch [9/10], Step [500/629], Loss: 0.1288\n",
      "Epoch [9/10], Step [600/629], Loss: 0.0261\n",
      "Epoch [10/10], Step [100/629], Loss: 0.0721\n",
      "Epoch [10/10], Step [200/629], Loss: 0.1166\n",
      "Epoch [10/10], Step [300/629], Loss: 0.0808\n",
      "Epoch [10/10], Step [400/629], Loss: 0.0933\n",
      "Epoch [10/10], Step [500/629], Loss: 0.0184\n",
      "Epoch [10/10], Step [600/629], Loss: 0.0999\n",
      "Test Accuracy: 96.67%\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 60]          47,100\n",
      "              ReLU-2                   [-1, 60]               0\n",
      "            Linear-3                   [-1, 10]             610\n",
      "================================================================\n",
      "Total params: 47,710\n",
      "Trainable params: 47,710\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.18\n",
      "Estimated Total Size (MB): 0.19\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set the device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 28 * 28  # MNIST image size is 28x28\n",
    "hidden_size = 60\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = MNIST(root='data/', train=True, transform=ToTensor(), download=True)\n",
    "\n",
    "# Split dataset into train and test\n",
    "train_size = int(0.67 * len(train_dataset))\n",
    "test_size = len(train_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(train_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the neural network architecture\n",
    "class TwoLayerNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.activation = nn.ReLU()  \n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.activation(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "model = TwoLayerNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Store accuracy in the dictionary with the three-tuple as the key\n",
    "accuracies[(2, hidden_size, 'r')] = accuracy\n",
    "\n",
    "# Print the summary of the model\n",
    "summary(model, (input_size,))\n",
    "\n",
    "# Plot accuracies\n",
    "if not isinstance(labels_graph, list):\n",
    "    labels_graph = labels_graph.tolist()\n",
    "labels_graph.append('2 Layers, 60 Nodes, ReLU')\n",
    "accuracies_values.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623a2701",
   "metadata": {},
   "source": [
    "## Two Layer net with 75 nodes in each layer and ReLU() activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a692cf12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/629], Loss: 0.5152\n",
      "Epoch [1/10], Step [200/629], Loss: 0.3752\n",
      "Epoch [1/10], Step [300/629], Loss: 0.3967\n",
      "Epoch [1/10], Step [400/629], Loss: 0.3040\n",
      "Epoch [1/10], Step [500/629], Loss: 0.2872\n",
      "Epoch [1/10], Step [600/629], Loss: 0.2283\n",
      "Epoch [2/10], Step [100/629], Loss: 0.3238\n",
      "Epoch [2/10], Step [200/629], Loss: 0.3316\n",
      "Epoch [2/10], Step [300/629], Loss: 0.2663\n",
      "Epoch [2/10], Step [400/629], Loss: 0.1705\n",
      "Epoch [2/10], Step [500/629], Loss: 0.3042\n",
      "Epoch [2/10], Step [600/629], Loss: 0.3705\n",
      "Epoch [3/10], Step [100/629], Loss: 0.1617\n",
      "Epoch [3/10], Step [200/629], Loss: 0.2152\n",
      "Epoch [3/10], Step [300/629], Loss: 0.1574\n",
      "Epoch [3/10], Step [400/629], Loss: 0.2802\n",
      "Epoch [3/10], Step [500/629], Loss: 0.0998\n",
      "Epoch [3/10], Step [600/629], Loss: 0.1490\n",
      "Epoch [4/10], Step [100/629], Loss: 0.3005\n",
      "Epoch [4/10], Step [200/629], Loss: 0.1111\n",
      "Epoch [4/10], Step [300/629], Loss: 0.0563\n",
      "Epoch [4/10], Step [400/629], Loss: 0.1504\n",
      "Epoch [4/10], Step [500/629], Loss: 0.1158\n",
      "Epoch [4/10], Step [600/629], Loss: 0.1002\n",
      "Epoch [5/10], Step [100/629], Loss: 0.1560\n",
      "Epoch [5/10], Step [200/629], Loss: 0.1207\n",
      "Epoch [5/10], Step [300/629], Loss: 0.1570\n",
      "Epoch [5/10], Step [400/629], Loss: 0.1073\n",
      "Epoch [5/10], Step [500/629], Loss: 0.0467\n",
      "Epoch [5/10], Step [600/629], Loss: 0.1907\n",
      "Epoch [6/10], Step [100/629], Loss: 0.1763\n",
      "Epoch [6/10], Step [200/629], Loss: 0.0990\n",
      "Epoch [6/10], Step [300/629], Loss: 0.0484\n",
      "Epoch [6/10], Step [400/629], Loss: 0.1443\n",
      "Epoch [6/10], Step [500/629], Loss: 0.0480\n",
      "Epoch [6/10], Step [600/629], Loss: 0.0267\n",
      "Epoch [7/10], Step [100/629], Loss: 0.1104\n",
      "Epoch [7/10], Step [200/629], Loss: 0.0661\n",
      "Epoch [7/10], Step [300/629], Loss: 0.0184\n",
      "Epoch [7/10], Step [400/629], Loss: 0.1525\n",
      "Epoch [7/10], Step [500/629], Loss: 0.0474\n",
      "Epoch [7/10], Step [600/629], Loss: 0.0596\n",
      "Epoch [8/10], Step [100/629], Loss: 0.0390\n",
      "Epoch [8/10], Step [200/629], Loss: 0.1236\n",
      "Epoch [8/10], Step [300/629], Loss: 0.0444\n",
      "Epoch [8/10], Step [400/629], Loss: 0.0861\n",
      "Epoch [8/10], Step [500/629], Loss: 0.0703\n",
      "Epoch [8/10], Step [600/629], Loss: 0.0676\n",
      "Epoch [9/10], Step [100/629], Loss: 0.0530\n",
      "Epoch [9/10], Step [200/629], Loss: 0.0520\n",
      "Epoch [9/10], Step [300/629], Loss: 0.0743\n",
      "Epoch [9/10], Step [400/629], Loss: 0.0691\n",
      "Epoch [9/10], Step [500/629], Loss: 0.1145\n",
      "Epoch [9/10], Step [600/629], Loss: 0.0209\n",
      "Epoch [10/10], Step [100/629], Loss: 0.0722\n",
      "Epoch [10/10], Step [200/629], Loss: 0.0594\n",
      "Epoch [10/10], Step [300/629], Loss: 0.0323\n",
      "Epoch [10/10], Step [400/629], Loss: 0.0623\n",
      "Epoch [10/10], Step [500/629], Loss: 0.0275\n",
      "Epoch [10/10], Step [600/629], Loss: 0.0207\n",
      "Test Accuracy: 96.66%\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 75]          58,875\n",
      "              ReLU-2                   [-1, 75]               0\n",
      "            Linear-3                   [-1, 10]             760\n",
      "================================================================\n",
      "Total params: 59,635\n",
      "Trainable params: 59,635\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.23\n",
      "Estimated Total Size (MB): 0.23\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set the device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 28 * 28  # MNIST image size is 28x28\n",
    "hidden_size = 75\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = MNIST(root='data/', train=True, transform=ToTensor(), download=True)\n",
    "\n",
    "# Split dataset into train and test\n",
    "train_size = int(0.67 * len(train_dataset))\n",
    "test_size = len(train_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(train_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the neural network architecture\n",
    "class TwoLayerNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.activation = nn.ReLU()  \n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.activation(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "model = TwoLayerNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Store accuracy in the dictionary with the three-tuple as the key\n",
    "accuracies[(2, hidden_size, 'r')] = accuracy\n",
    "\n",
    "# Print the summary of the model\n",
    "summary(model, (input_size,))\n",
    "\n",
    "# Plot accuracies\n",
    "if not isinstance(labels_graph, list):\n",
    "    labels_graph = labels_graph.tolist()\n",
    "labels_graph.append('2 Layers, 75 Nodes, ReLU')\n",
    "accuracies_values.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538e339d",
   "metadata": {},
   "source": [
    "## Three Layer net with 33 nodes in each and ReLU() as activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f48258ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/629], Loss: 0.6247\n",
      "Epoch [1/10], Step [200/629], Loss: 0.3720\n",
      "Epoch [1/10], Step [300/629], Loss: 0.4193\n",
      "Epoch [1/10], Step [400/629], Loss: 0.3386\n",
      "Epoch [1/10], Step [500/629], Loss: 0.4295\n",
      "Epoch [1/10], Step [600/629], Loss: 0.2586\n",
      "Epoch [2/10], Step [100/629], Loss: 0.1348\n",
      "Epoch [2/10], Step [200/629], Loss: 0.2430\n",
      "Epoch [2/10], Step [300/629], Loss: 0.1327\n",
      "Epoch [2/10], Step [400/629], Loss: 0.2883\n",
      "Epoch [2/10], Step [500/629], Loss: 0.3272\n",
      "Epoch [2/10], Step [600/629], Loss: 0.1273\n",
      "Epoch [3/10], Step [100/629], Loss: 0.1212\n",
      "Epoch [3/10], Step [200/629], Loss: 0.2248\n",
      "Epoch [3/10], Step [300/629], Loss: 0.1170\n",
      "Epoch [3/10], Step [400/629], Loss: 0.2105\n",
      "Epoch [3/10], Step [500/629], Loss: 0.0773\n",
      "Epoch [3/10], Step [600/629], Loss: 0.1741\n",
      "Epoch [4/10], Step [100/629], Loss: 0.2142\n",
      "Epoch [4/10], Step [200/629], Loss: 0.1017\n",
      "Epoch [4/10], Step [300/629], Loss: 0.1192\n",
      "Epoch [4/10], Step [400/629], Loss: 0.1864\n",
      "Epoch [4/10], Step [500/629], Loss: 0.0889\n",
      "Epoch [4/10], Step [600/629], Loss: 0.3215\n",
      "Epoch [5/10], Step [100/629], Loss: 0.1884\n",
      "Epoch [5/10], Step [200/629], Loss: 0.1246\n",
      "Epoch [5/10], Step [300/629], Loss: 0.2064\n",
      "Epoch [5/10], Step [400/629], Loss: 0.1175\n",
      "Epoch [5/10], Step [500/629], Loss: 0.2549\n",
      "Epoch [5/10], Step [600/629], Loss: 0.2214\n",
      "Epoch [6/10], Step [100/629], Loss: 0.0992\n",
      "Epoch [6/10], Step [200/629], Loss: 0.1759\n",
      "Epoch [6/10], Step [300/629], Loss: 0.1435\n",
      "Epoch [6/10], Step [400/629], Loss: 0.3425\n",
      "Epoch [6/10], Step [500/629], Loss: 0.0595\n",
      "Epoch [6/10], Step [600/629], Loss: 0.0376\n",
      "Epoch [7/10], Step [100/629], Loss: 0.0421\n",
      "Epoch [7/10], Step [200/629], Loss: 0.2904\n",
      "Epoch [7/10], Step [300/629], Loss: 0.0691\n",
      "Epoch [7/10], Step [400/629], Loss: 0.1063\n",
      "Epoch [7/10], Step [500/629], Loss: 0.2886\n",
      "Epoch [7/10], Step [600/629], Loss: 0.1098\n",
      "Epoch [8/10], Step [100/629], Loss: 0.0602\n",
      "Epoch [8/10], Step [200/629], Loss: 0.0855\n",
      "Epoch [8/10], Step [300/629], Loss: 0.0739\n",
      "Epoch [8/10], Step [400/629], Loss: 0.1850\n",
      "Epoch [8/10], Step [500/629], Loss: 0.0996\n",
      "Epoch [8/10], Step [600/629], Loss: 0.0948\n",
      "Epoch [9/10], Step [100/629], Loss: 0.1249\n",
      "Epoch [9/10], Step [200/629], Loss: 0.1814\n",
      "Epoch [9/10], Step [300/629], Loss: 0.2100\n",
      "Epoch [9/10], Step [400/629], Loss: 0.1280\n",
      "Epoch [9/10], Step [500/629], Loss: 0.0765\n",
      "Epoch [9/10], Step [600/629], Loss: 0.0886\n",
      "Epoch [10/10], Step [100/629], Loss: 0.2052\n",
      "Epoch [10/10], Step [200/629], Loss: 0.1815\n",
      "Epoch [10/10], Step [300/629], Loss: 0.0974\n",
      "Epoch [10/10], Step [400/629], Loss: 0.0170\n",
      "Epoch [10/10], Step [500/629], Loss: 0.0820\n",
      "Epoch [10/10], Step [600/629], Loss: 0.1977\n",
      "Test Accuracy: 95.70%\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 33]          25,905\n",
      "              ReLU-2                   [-1, 33]               0\n",
      "            Linear-3                   [-1, 33]           1,122\n",
      "              ReLU-4                   [-1, 33]               0\n",
      "            Linear-5                   [-1, 10]             340\n",
      "================================================================\n",
      "Total params: 27,367\n",
      "Trainable params: 27,367\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.10\n",
      "Estimated Total Size (MB): 0.11\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set the device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 28 * 28  # MNIST image size is 28x28\n",
    "hidden_size = 33\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = MNIST(root='data/', train=True, transform=ToTensor(), download=True)\n",
    "\n",
    "# Split dataset into train and test\n",
    "train_size = int(0.67 * len(train_dataset))\n",
    "test_size = len(train_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(train_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the neural network architecture\n",
    "class ThreeLayerNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(ThreeLayerNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.activation = nn.ReLU()  \n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.activation(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "model = ThreeLayerNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Store accuracy in the dictionary with the three-tuple as the key\n",
    "accuracies[(3, hidden_size, 'r')] = accuracy\n",
    "\n",
    "# Print the summary of the model\n",
    "summary(model, (input_size,))\n",
    "\n",
    "# Plot accuracies\n",
    "if not isinstance(labels_graph, list):\n",
    "    labels_graph = labels_graph.tolist()\n",
    "labels_graph.append('3 Layers, 33 Nodes, ReLU')\n",
    "accuracies_values.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7c7f57",
   "metadata": {},
   "source": [
    "## Three Layer net with 50 nodes in each and ReLU() as activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb767f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/629], Loss: 0.5186\n",
      "Epoch [1/10], Step [200/629], Loss: 0.4589\n",
      "Epoch [1/10], Step [300/629], Loss: 0.3980\n",
      "Epoch [1/10], Step [400/629], Loss: 0.2521\n",
      "Epoch [1/10], Step [500/629], Loss: 0.1933\n",
      "Epoch [1/10], Step [600/629], Loss: 0.1809\n",
      "Epoch [2/10], Step [100/629], Loss: 0.2337\n",
      "Epoch [2/10], Step [200/629], Loss: 0.3118\n",
      "Epoch [2/10], Step [300/629], Loss: 0.0975\n",
      "Epoch [2/10], Step [400/629], Loss: 0.3494\n",
      "Epoch [2/10], Step [500/629], Loss: 0.1434\n",
      "Epoch [2/10], Step [600/629], Loss: 0.2937\n",
      "Epoch [3/10], Step [100/629], Loss: 0.2201\n",
      "Epoch [3/10], Step [200/629], Loss: 0.2463\n",
      "Epoch [3/10], Step [300/629], Loss: 0.0633\n",
      "Epoch [3/10], Step [400/629], Loss: 0.0955\n",
      "Epoch [3/10], Step [500/629], Loss: 0.0510\n",
      "Epoch [3/10], Step [600/629], Loss: 0.1555\n",
      "Epoch [4/10], Step [100/629], Loss: 0.3292\n",
      "Epoch [4/10], Step [200/629], Loss: 0.1614\n",
      "Epoch [4/10], Step [300/629], Loss: 0.0998\n",
      "Epoch [4/10], Step [400/629], Loss: 0.2512\n",
      "Epoch [4/10], Step [500/629], Loss: 0.1393\n",
      "Epoch [4/10], Step [600/629], Loss: 0.1454\n",
      "Epoch [5/10], Step [100/629], Loss: 0.1645\n",
      "Epoch [5/10], Step [200/629], Loss: 0.1284\n",
      "Epoch [5/10], Step [300/629], Loss: 0.1085\n",
      "Epoch [5/10], Step [400/629], Loss: 0.0915\n",
      "Epoch [5/10], Step [500/629], Loss: 0.1573\n",
      "Epoch [5/10], Step [600/629], Loss: 0.1146\n",
      "Epoch [6/10], Step [100/629], Loss: 0.0938\n",
      "Epoch [6/10], Step [200/629], Loss: 0.0298\n",
      "Epoch [6/10], Step [300/629], Loss: 0.0571\n",
      "Epoch [6/10], Step [400/629], Loss: 0.0769\n",
      "Epoch [6/10], Step [500/629], Loss: 0.1889\n",
      "Epoch [6/10], Step [600/629], Loss: 0.0585\n",
      "Epoch [7/10], Step [100/629], Loss: 0.0592\n",
      "Epoch [7/10], Step [200/629], Loss: 0.0532\n",
      "Epoch [7/10], Step [300/629], Loss: 0.1092\n",
      "Epoch [7/10], Step [400/629], Loss: 0.1724\n",
      "Epoch [7/10], Step [500/629], Loss: 0.0331\n",
      "Epoch [7/10], Step [600/629], Loss: 0.1535\n",
      "Epoch [8/10], Step [100/629], Loss: 0.0684\n",
      "Epoch [8/10], Step [200/629], Loss: 0.0938\n",
      "Epoch [8/10], Step [300/629], Loss: 0.0668\n",
      "Epoch [8/10], Step [400/629], Loss: 0.1301\n",
      "Epoch [8/10], Step [500/629], Loss: 0.0683\n",
      "Epoch [8/10], Step [600/629], Loss: 0.0409\n",
      "Epoch [9/10], Step [100/629], Loss: 0.0682\n",
      "Epoch [9/10], Step [200/629], Loss: 0.0162\n",
      "Epoch [9/10], Step [300/629], Loss: 0.0081\n",
      "Epoch [9/10], Step [400/629], Loss: 0.0444\n",
      "Epoch [9/10], Step [500/629], Loss: 0.0425\n",
      "Epoch [9/10], Step [600/629], Loss: 0.0247\n",
      "Epoch [10/10], Step [100/629], Loss: 0.0677\n",
      "Epoch [10/10], Step [200/629], Loss: 0.0160\n",
      "Epoch [10/10], Step [300/629], Loss: 0.0355\n",
      "Epoch [10/10], Step [400/629], Loss: 0.0306\n",
      "Epoch [10/10], Step [500/629], Loss: 0.1242\n",
      "Epoch [10/10], Step [600/629], Loss: 0.0979\n",
      "Test Accuracy: 95.83%\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 50]          39,250\n",
      "              ReLU-2                   [-1, 50]               0\n",
      "            Linear-3                   [-1, 50]           2,550\n",
      "              ReLU-4                   [-1, 50]               0\n",
      "            Linear-5                   [-1, 10]             510\n",
      "================================================================\n",
      "Total params: 42,310\n",
      "Trainable params: 42,310\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.16\n",
      "Estimated Total Size (MB): 0.17\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set the device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 28 * 28  # MNIST image size is 28x28\n",
    "hidden_size = 50\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = MNIST(root='data/', train=True, transform=ToTensor(), download=True)\n",
    "\n",
    "# Split dataset into train and test\n",
    "train_size = int(0.67 * len(train_dataset))\n",
    "test_size = len(train_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(train_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the neural network architecture\n",
    "class ThreeLayerNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(ThreeLayerNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.activation = nn.ReLU()  \n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.activation(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "model = ThreeLayerNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Store accuracy in the dictionary with the three-tuple as the key\n",
    "accuracies[(3, hidden_size, 'r')] = accuracy\n",
    "\n",
    "# Print the summary of the model\n",
    "summary(model, (input_size,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69195ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracies\n",
    "if not isinstance(labels_graph, list):\n",
    "    labels_graph = labels_graph.tolist()\n",
    "labels_graph.append('3 Layers, 50 Nodes, ReLU')\n",
    "accuracies_values.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "149c31ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmYAAAGDCAYAAACBTdwmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAc4UlEQVR4nO3debRdZZ3m8e8jEZmkIRiQUbRkUpaCpmzUKtsSbScEqm0aUDCrHFhW2xTaTmDVKstSa7Es26HLdqBUiIogAgqFQ4FxoHQpGsAJQXGESICAIAiiDL/+Y+/oyc29yUVzzn7J+X7Wyjpnj+d33pt78uR937N3qgpJkiQN735DFyBJkqSOwUySJKkRBjNJkqRGGMwkSZIaYTCTJElqhMFMkiSpEQYzSbqPSvKZJEuGrkPShmMwkzSnJF9MclOSBwxdy7gk2TrJO5JcleRXSX7YLz9o6NrWp6qeWVVLh65D0oZjMJM0qyS7A38OFHDwhF97wYReZ1NgGfBI4BnA1sATgBuBx02ihj9EOn5+Sxshf7ElzeUFwNeAU4A1hsuS7Jrk7CSrktyY5F0j216S5PIktyb5XpLH9OsrycNH9jslyZv6509OsiLJa5NcC5ycZNsk5/WvcVP/fJeR4xcmOTnJNf32T/brv5vkOSP73T/JDUn2m+M97gb8ZVV9r6ruqarrq+qNVfXp/vh9+p7Dm5NcluTgkXOfkuTd/ZDir5J8JcmD+x63m5JckWT/kf1/muSEvl1u6uvfrN+2vvf7xSRvTvIV4HbgYf26F/fbH57kS0l+2b/fj40c+4Qk3+i3fSPJE2ac94197bcmOf++0FsobawMZpLm8gLg1P7P05PsAJBkE+A84GfA7sDOwOn9tsOAf+iP3Zqup+3Geb7eg4GFwEOAY+g+n07ul3cDfg28a2T/DwNb0PV2bQ+8vV//IeCokf2eBaysqm/O8ppPBT5bVb+araAk9wf+DTi/f41jgVOT7DWy2/8A/g54EPAb4KvAJf3ymcDbZpz2+cDTgT8B9uyPZR7vF+BourZ5IF37j3pjX+e2wC7Av/TvYSHwKeD/Atv19XwqyXYjxz4P+Kv+PW4KvGq29pA0fgYzSWtJ8md0AeGMqroY+BHdP97QDfHtBLy6qm6rqjuq6sv9thcDb6mqb1Tnh1U1M0DM5R7g9VX1m6r6dVXdWFVnVdXtVXUr8Gbgv/T17Qg8E3hpVd1UVXdW1Zf683wEeFaSrfvlo+lC3Gy2A1auo6YDgK2AE6vqt1X1ebpQeuTIPp+oqour6g7gE8AdVfWhqrob+Biw/4xzvquqrq6qX/Tv6UiAdb3fEadU1WVVdVdV3Tlj2510P7OdZvxMng1cWVUf7o87DbgCeM7IsSdX1Q+q6tfAGcB+62gTSWNkMJM0myXA+VV1Q7/8UX4/nLkr8LOqumuW43alC3F/iFV9uAEgyRZJ3pfkZ0luAS4Etul77HYFflFVN808SVVdA3wFeG6SbegC3KlzvOaNwI7rqGkn4Oqqumdk3c/oeglXu27k+a9nWd5qxjmvnnGunWC973e2Y2d6DRDg6/2Q6wtH3sPMcDzzPVw78vz2WWqWNCETmWAr6b4jyeZ0w3Ob9PO9AB5AFxIeTRcOdkuyYJZwdjXdEN1sbqcbelztwcCKkeWasf8rgb2A/1xV1/ZzxC6lCx9XAwuTbFNVN8/yWkvpeu8WAF+tqp/PUdPngDcl2bKqbptl+zXArknuNxLOdgN+MMf55mPXkee79a8B636/q81so99vqLoWeAn8rsfzc0ku7M//kBm77wZ89o94D5LGxB4zSTMdCtwNPIJuSGs/YB/gP+jmjn2dbvjvxCRbJtksyRP7Y98PvCrJY9N5eJLVoeCbwPOSbJLkGaw9TDfTA+l6nG7u50m9fvWGqloJfAZ4dz9p/v5JnjRy7CeBxwDH0c05m8uH6ULeWUn2TnK/JNsleV2SZwEXAbcBr+lf48l0Q4Cnr6f2dXlZkl369/Q6uuHOdb7f+Uhy2MiXBW6iC3F3A58G9kzyvCQLkhxO97M97494D5LGxGAmaaYldHOOrqqqa1f/oZuI/ny6HpznAA8HrqLr9TocoKo+Tjc36qPArXQBaWF/3uP6427uz/PJ9dTxDmBz4Aa6b4fO7OE5mm5e1RXA9cDLV2/o50qdBTwUOHuuF6iq39B9AeAK4ALgFrrg+SDgoqr6Ld0XGJ7Z1/Fu4AVVdcV6al+Xj9JN0v9x/+dN/fp3sO73uz5/ClyU5FfAucBxVfWTqroROIiuR+5GuiHPg0aGqSU1JFVz9oxL0n1Wkr8H9qyqo9a784Qk+Snw4qr63NC1SGqTc8wkbXT6ocAX0fWqSdJ9xtiGMpN8MMn1Sb47sm5hkguSXNk/bjuy7YR0t0L5fpKnj6suSRu3JC+hmzf2maq6cOh6JOneGNtQZj8R91fAh6pq337dW+i+4n5ikuOBbavqtUkeAZzG76+P9Dm6IYi7x1KcJElSg8bWY9b/T/UXM1YfQvc1dvrHQ0fWn95fWPInwA9p+D51kiRJ4zDpb2Xu0H/NffXX3bfv1+/MmhdOXMGaFz+UJEna6LUy+T+zrJt1jDXJMXT3imPLLbd87N577z3OuiRJkjaIiy+++IaqWrSufSYdzK5LsmNVrezvdXd9v34Fa14Nexd+fzXsNVTVScBJAIsXL67ly5ePs15JkqQNIsl67x086aHMc/n9/faWAOeMrD8iyQOSPBTYg+4ij5IkSVNjbD1mSU4Dngw8KMkKutuLnAickeRFdFcMPwygqi5LcgbwPeAu4GV+I1OSJE2bsQWzqjpyjk0HzrH/m+lu5SJJkjSVvFemJElSIwxmkiRJjTCYSZIkNcJgJkmS1AiDmSRJUiMMZpIkSY1o5ZZMTdv9+E9N7LV+euKzrWOGddXSilZ+NtJ9RUufI638/rZSh4ZlMNN9mh9kkqSNicFM0gZnb0i7WvrZSOszjb+/BjNpIzKNH2KSNizD+7AMZpI0AYZmSfPhtzIlSZIaYTCTJElqhMFMkiSpEQYzSZKkRhjMJEmSGmEwkyRJaoTBTJIkqREGM0mSpEYYzCRJkhphMJMkSWqEwUySJKkRBjNJkqRGGMwkSZIaYTCTJElqhMFMkiSpEQYzSZKkRhjMJEmSGmEwkyRJaoTBTJIkqREGM0mSpEYYzCRJkhphMJMkSWqEwUySJKkRBjNJkqRGGMwkSZIaYTCTJElqhMFMkiSpEQYzSZKkRhjMJEmSGmEwkyRJaoTBTJIkqREGM0mSpEYYzCRJkhphMJMkSWqEwUySJKkRBjNJkqRGGMwkSZIaYTCTJElqhMFMkiSpEQYzSZKkRhjMJEmSGmEwkyRJaoTBTJIkqREGM0mSpEYYzCRJkhoxSDBL8ooklyX5bpLTkmyWZGGSC5Jc2T9uO0RtkiRJQ5l4MEuyM/A3wOKq2hfYBDgCOB5YVlV7AMv6ZUmSpKkx1FDmAmDzJAuALYBrgEOApf32pcChw5QmSZI0jIkHs6r6OfBW4CpgJfDLqjof2KGqVvb7rAS2n3RtkiRJQxpiKHNbut6xhwI7AVsmOepeHH9MkuVJlq9atWpcZUqSJE3cEEOZTwV+UlWrqupO4GzgCcB1SXYE6B+vn+3gqjqpqhZX1eJFixZNrGhJkqRxGyKYXQUckGSLJAEOBC4HzgWW9PssAc4ZoDZJkqTBLJj0C1bVRUnOBC4B7gIuBU4CtgLOSPIiuvB22KRrkyRJGtLEgxlAVb0eeP2M1b+h6z2TJEmaSl75X5IkqREGM0mSpEYYzCRJkhphMJMkSWqEwUySJKkRBjNJkqRGGMwkSZIaYTCTJElqhMFMkiSpEQYzSZKkRhjMJEmSGmEwkyRJaoTBTJIkqREGM0mSpEYYzCRJkhphMJMkSWqEwUySJKkRBjNJkqRGGMwkSZIaYTCTJElqhMFMkiSpEQYzSZKkRhjMJEmSGmEwkyRJaoTBTJIkqREGM0mSpEYYzCRJkhphMJMkSWqEwUySJKkRBjNJkqRGGMwkSZIaYTCTJElqhMFMkiSpEQYzSZKkRhjMJEmSGmEwkyRJaoTBTJIkqREGM0mSpEYYzCRJkhphMJMkSWqEwUySJKkRBjNJkqRGGMwkSZIaYTCTJElqhMFMkiSpEQYzSZKkRhjMJEmSGmEwkyRJaoTBTJIkqREGM0mSpEYYzCRJkhphMJMkSWqEwUySJKkRBjNJkqRGGMwkSZIaYTCTJElqhMFMkiSpEYMEsyTbJDkzyRVJLk/y+CQLk1yQ5Mr+cdshapMkSRrKUD1m7wQ+W1V7A48GLgeOB5ZV1R7Asn5ZkiRpakw8mCXZGngS8AGAqvptVd0MHAIs7XdbChw66dokSZKGNESP2cOAVcDJSS5N8v4kWwI7VNVKgP5x+9kOTnJMkuVJlq9atWpyVUuSJI3ZEMFsAfAY4D1VtT9wG/di2LKqTqqqxVW1eNGiReOqUZIkaeKGCGYrgBVVdVG/fCZdULsuyY4A/eP1A9QmSZI0mIkHs6q6Frg6yV79qgOB7wHnAkv6dUuAcyZdmyRJ0pAWDPS6xwKnJtkU+DHwV3Qh8YwkLwKuAg4bqDZJkqRBDBLMquqbwOJZNh044VIkSZKa4ZX/JUmSGmEwkyRJaoTBTJIkqREGM0mSpEYYzCRJkhphMJMkSWrEeoNZkoOSGOAkSZLGbD6B6wjgyiRvSbLPuAuSJEmaVusNZlV1FLA/8CPg5CRfTXJMkgeOvTpJkqQpMq8hyqq6BTgLOB3YEfhL4JIkx46xNkmSpKkynzlmz0nyCeDzwP2Bx1XVM4FHA68ac32SJElTYz73yjwMeHtVXTi6sqpuT/LC8ZQlSZI0feYTzF4PrFy9kGRzYIeq+mlVLRtbZZIkSVNmPnPMPg7cM7J8d79OkiRJG9B8gtmCqvrt6oX++abjK0mSJGk6zSeYrUpy8OqFJIcAN4yvJEmSpOk0nzlmLwVOTfIuIMDVwAvGWpUkSdIUWm8wq6ofAQck2QpIVd06/rIkSZKmz3x6zEjybOCRwGZJAKiqfxxjXZIkSVNnPheYfS9wOHAs3VDmYcBDxlyXJEnS1JnP5P8nVNULgJuq6g3A44Fdx1uWJEnS9JlPMLujf7w9yU7AncBDx1eSJEnSdJrPHLN/S7IN8M/AJUAB/zrOoiRJkqbROoNZkvsBy6rqZuCsJOcBm1XVLydRnCRJ0jRZ51BmVd0D/J+R5d8YyiRJksZjPnPMzk/y3Ky+ToYkSZLGYj5zzP43sCVwV5I76C6ZUVW19VgrkyRJmjLzufL/AydRiCRJ0rRbbzBL8qTZ1lfVhRu+HEmSpOk1n6HMV4883wx4HHAx8JSxVCRJkjSl5jOU+ZzR5SS7Am8ZW0WSJElTaj7fypxpBbDvhi5EkiRp2s1njtm/0F3tH7ogtx/wrTHWJEmSNJXmM8ds+cjzu4DTquorY6pHkiRpas0nmJ0J3FFVdwMk2STJFlV1+3hLkyRJmi7zmWO2DNh8ZHlz4HPjKUeSJGl6zSeYbVZVv1q90D/fYnwlSZIkTaf5BLPbkjxm9UKSxwK/Hl9JkiRJ02k+c8xeDnw8yTX98o7A4WOrSJIkaUrN5wKz30iyN7AX3Q3Mr6iqO8demSRJ0pRZ71BmkpcBW1bVd6vqO8BWSf7n+EuTJEmaLvOZY/aSqrp59UJV3QS8ZGwVSZIkTan5BLP7JcnqhSSbAJuOryRJkqTpNJ/J//8OnJHkvXS3Znop8JmxViVJkjSF5hPMXgscA/w13eT/S+m+mSlJkqQNaL1DmVV1D/A14MfAYuBA4PIx1yVJkjR15uwxS7IncARwJHAj8DGAqvqLyZQmSZI0XdY1lHkF8B/Ac6rqhwBJXjGRqiRJkqbQuoYynwtcC3whyb8mOZBujpkkSZLGYM5gVlWfqKrDgb2BLwKvAHZI8p4k/3VC9UmSJE2N+Uz+v62qTq2qg4BdgG8Cx4+7MEmSpGkznwvM/k5V/aKq3ldVTxlXQZIkSdPqXgUzSZIkjY/BTJIkqREGM0mSpEYYzCRJkhoxWDBLskmSS5Oc1y8vTHJBkiv7x22Hqk2SJGkIQ/aYHcea99w8HlhWVXsAy/CSHJIkacoMEsyS7AI8G3j/yOpDgKX986XAoRMuS5IkaVBD9Zi9A3gNcM/Iuh2qaiVA/7j9bAcmOSbJ8iTLV61aNfZCJUmSJmXiwSzJQcD1VXXxH3J8VZ1UVYuravGiRYs2cHWSJEnDWTDAaz4RODjJs4DNgK2TfAS4LsmOVbUyyY7A9QPUJkmSNJiJ95hV1QlVtUtV7Q4cAXy+qo4CzgWW9LstAc6ZdG2SJElDauk6ZicCT0tyJfC0flmSJGlqDDGU+TtV9UXgi/3zG4EDh6xHkiRpSC31mEmSJE01g5kkSVIjDGaSJEmNMJhJkiQ1wmAmSZLUCIOZJElSIwxmkiRJjTCYSZIkNcJgJkmS1AiDmSRJUiMMZpIkSY0wmEmSJDXCYCZJktQIg5kkSVIjDGaSJEmNMJhJkiQ1wmAmSZLUCIOZJElSIwxmkiRJjTCYSZIkNcJgJkmS1AiDmSRJUiMMZpIkSY0wmEmSJDXCYCZJktQIg5kkSVIjDGaSJEmNMJhJkiQ1wmAmSZLUCIOZJElSIwxmkiRJjTCYSZIkNcJgJkmS1AiDmSRJUiMMZpIkSY0wmEmSJDXCYCZJktQIg5kkSVIjDGaSJEmNMJhJkiQ1wmAmSZLUCIOZJElSIwxmkiRJjTCYSZIkNcJgJkmS1AiDmSRJUiMMZpIkSY0wmEmSJDXCYCZJktQIg5kkSVIjDGaSJEmNMJhJkiQ1wmAmSZLUCIOZJElSIwxmkiRJjTCYSZIkNWLiwSzJrkm+kOTyJJclOa5fvzDJBUmu7B+3nXRtkiRJQxqix+wu4JVVtQ9wAPCyJI8AjgeWVdUewLJ+WZIkaWpMPJhV1cqquqR/fitwObAzcAiwtN9tKXDopGuTJEka0qBzzJLsDuwPXATsUFUroQtvwPYDliZJkjRxgwWzJFsBZwEvr6pb7sVxxyRZnmT5qlWrxlegJEnShA0SzJLcny6UnVpVZ/err0uyY799R+D62Y6tqpOqanFVLV60aNFkCpYkSZqAIb6VGeADwOVV9baRTecCS/rnS4BzJl2bJEnSkBYM8JpPBI4GvpPkm/261wEnAmckeRFwFXDYALVJkiQNZuLBrKq+DGSOzQdOshZJkqSWeOV/SZKkRhjMJEmSGmEwkyRJaoTBTJIkqREGM0mSpEYYzCRJkhphMJMkSWqEwUySJKkRBjNJkqRGGMwkSZIaYTCTJElqhMFMkiSpEQYzSZKkRhjMJEmSGmEwkyRJaoTBTJIkqREGM0mSpEYYzCRJkhphMJMkSWqEwUySJKkRBjNJkqRGGMwkSZIaYTCTJElqhMFMkiSpEQYzSZKkRhjMJEmSGmEwkyRJaoTBTJIkqREGM0mSpEYYzCRJkhphMJMkSWqEwUySJKkRBjNJkqRGGMwkSZIaYTCTJElqhMFMkiSpEQYzSZKkRhjMJEmSGmEwkyRJaoTBTJIkqREGM0mSpEYYzCRJkhphMJMkSWqEwUySJKkRBjNJkqRGGMwkSZIaYTCTJElqhMFMkiSpEQYzSZKkRhjMJEmSGmEwkyRJaoTBTJIkqREGM0mSpEYYzCRJkhphMJMkSWqEwUySJKkRzQWzJM9I8v0kP0xy/ND1SJIkTUpTwSzJJsD/A54JPAI4Mskjhq1KkiRpMpoKZsDjgB9W1Y+r6rfA6cAhA9ckSZI0Ea0Fs52Bq0eWV/TrJEmSNnqpqqFr+J0khwFPr6oX98tHA4+rqmNH9jkGOKZf3Av4/ozTPAi4YQLl3pfYJmuyPdZmm6zJ9libbbIm22NttsnaZrbJQ6pq0boOWDDeeu61FcCuI8u7ANeM7lBVJwEnzXWCJMuravF4yrtvsk3WZHuszTZZk+2xNttkTbbH2myTtf0hbdLaUOY3gD2SPDTJpsARwLkD1yRJkjQRTfWYVdVdSf4X8O/AJsAHq+qygcuSJEmaiKaCGUBVfRr49B9xijmHOaeYbbIm22NttsmabI+12SZrsj3WZpus7V63SVOT/yVJkqZZa3PMJEmSptZGFcy8ndPvJdk1yReSXJ7ksiTHDV1TC5JskuTSJOcNXUsLkmyT5MwkV/R/Vx4/dE1DS/KK/nfmu0lOS7LZ0DVNWpIPJrk+yXdH1i1MckGSK/vHbYescZLmaI9/7n9vvp3kE0m2GbDEiZutTUa2vSpJJXnQELUNYa72SHJsn0suS/KW+Zxrowlm3s5pLXcBr6yqfYADgJdNeXusdhxw+dBFNOSdwGeram/g0Ux52yTZGfgbYHFV7Uv3JaQjhq1qEKcAz5ix7nhgWVXtASzrl6fFKazdHhcA+1bVo4AfACdMuqiBncLabUKSXYGnAVdNuqCBncKM9kjyF3R3L3pUVT0SeOt8TrTRBDO8ndMaqmplVV3SP7+V7h/cqb6LQpJdgGcD7x+6lhYk2Rp4EvABgKr6bVXdPGhRbVgAbJ5kAbAFM66lOA2q6kLgFzNWHwIs7Z8vBQ6dZE1Dmq09qur8qrqrX/wa3XU3p8Ycf0cA3g68BpiqCexztMdfAydW1W/6fa6fz7k2pmDm7ZzmkGR3YH/gooFLGdo76D4w7hm4jlY8DFgFnNwP774/yZZDFzWkqvo53f9qrwJWAr+sqvOHraoZO1TVSuj+4wdsP3A9LXkh8JmhixhakoOBn1fVt4aupRF7An+e5KIkX0ryp/M5aGMKZpll3VQl9tkk2Qo4C3h5Vd0ydD1DSXIQcH1VXTx0LQ1ZADwGeE9V7Q/cxnQNT62lnzd1CPBQYCdgyyRHDVuVWpbkb+mmjpw6dC1DSrIF8LfA3w9dS0MWANvSTSd6NXBGktmyyho2pmC23ts5TZsk96cLZadW1dlD1zOwJwIHJ/kp3TD3U5J8ZNiSBrcCWFFVq3tSz6QLatPsqcBPqmpVVd0JnA08YeCaWnFdkh0B+sd5DctszJIsAQ4Cnl9ee+pP6P5D863+c3YX4JIkDx60qmGtAM6uztfpRmvW+4WIjSmYeTunEX0q/wBweVW9beh6hlZVJ1TVLlW1O93fjc9X1VT3hFTVtcDVSfbqVx0IfG/AklpwFXBAki3636EDmfIvRIw4F1jSP18CnDNgLYNL8gzgtcDBVXX70PUMraq+U1XbV9Xu/efsCuAx/efMtPok8BSAJHsCmzKPm7xvNMGsn4S5+nZOlwNnTPntnJ4IHE3XM/TN/s+zhi5KzTkWODXJt4H9gH8atpxh9b2HZwKXAN+h+4ycuquZJzkN+CqwV5IVSV4EnAg8LcmVdN+6O3HIGidpjvZ4F/BA4IL+8/W9gxY5YXO0ydSaoz0+CDysv4TG6cCS+fSseuV/SZKkRmw0PWaSJEn3dQYzSZKkRhjMJEmSGmEwkyRJaoTBTJIkqREGM0kbrSSV5MMjywuSrEpy3r08z0+TrPPCkPPZR5LWx2AmaWN2G7Bvks375acBPx+wHklaJ4OZpI3dZ4Bn98+PBE5bvSHJwiSfTPLtJF9L8qh+/XZJzu9v7v4+Ru7Fm+SoJF/vLyr6viSbTPLNSNq4GcwkbexOB45IshnwKOCikW1vAC6tqkcBrwM+1K9/PfDl/ubu5wK7ASTZBzgceGJV7QfcDTx/Em9C0nRYMHQBkjROVfXtJLvT9ZZ9esbmPwOe2+/3+b6n7D8BTwL+W7/+U0lu6vc/EHgs8I3uVppsjjfzlrQBGcwkTYNzgbcCTwa2G1mfWfatGY+jAiytqhM2aHWS1HMoU9I0+CDwj1X1nRnrL6QfikzyZOCGqrplxvpnAtv2+y8D/nuS7fttC5M8ZOzVS5oa9phJ2uhV1QrgnbNs+gfg5CTfBm4HlvTr3wCcluQS4EvAVf15vpfk74Dzk9wPuBN4GfCz8b4DSdMiVbP11kuSJGnSHMqUJElqhMFMkiSpEQYzSZKkRhjMJEmSGmEwkyRJaoTBTJIkqREGM0mSpEYYzCRJkhrx/wESajzKSS8bngAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert labels and accuracies_values to lists\n",
    "if not isinstance(labels, list):\n",
    "    labels = labels.tolist()\n",
    "if not isinstance(accuracies_values, list):\n",
    "    accuracies_values = accuracies_values.tolist()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(1,16), accuracies_values)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Comparison')\n",
    "plt.ylim([0, 100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "25d70633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1: 2 Layers, 50 Nodes, Tanh, Accuracy: 96.41%\n",
      "Model 2: 2 Layers, 60 Nodes, Tanh, Accuracy: 96.63%\n",
      "Model 3: 2 Layers, 75 Nodes, Tanh, Accuracy: 96.54%\n",
      "Model 4: 3 Layers, 33 Nodes, Tanh, Accuracy: 96.10%\n",
      "Model 5: 3 Layers, 50 Nodes, Tanh, Accuracy: 96.62%\n",
      "Model 6: 2 Layers, 50 Nodes, Sigmoid, Accuracy: 95.53%\n",
      "Model 7: 2 Layers, 60 Nodes, Sigmoid, Accuracy: 96.01%\n",
      "Model 8: 2 Layers, 75 Nodes, Sigmoid, Accuracy: 95.91%\n",
      "Model 9: 3 Layers, 33 Nodes, Sigmoid, Accuracy: 95.17%\n",
      "Model 10: 3 Layers, 50 Nodes, Sigmoid, Accuracy: 95.86%\n",
      "Model 11: 2 Layers, 50 Nodes, ReLU, Accuracy: 96.46%\n",
      "Model 12: 2 Layers, 60 Nodes, ReLU, Accuracy: 96.67%\n",
      "Model 13: 2 Layers, 75 Nodes, ReLU, Accuracy: 96.66%\n",
      "Model 14: 3 Layers, 33 Nodes, ReLU, Accuracy: 95.70%\n",
      "Model 15: 3 Layers, 50 Nodes, ReLU, Accuracy: 95.83%\n"
     ]
    }
   ],
   "source": [
    "for i in range (1, 16):\n",
    "    print(f\"Model {i}: {labels_graph[i-1]}, Accuracy: {accuracies_values[i-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "003c15e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: (2 Layers, 60 Nodes, ReLU) with accuracy: 96.67%\n",
      "Worst Model: (3 Layers, 33 Nodes, Sigmoid) with accuracy: 95.17%\n",
      "Difference in accuracies: 1.49%\n",
      "Standard Deviation in accuracies: 0.45%\n",
      "The difference is equivalant to 3.29 standard deviations\n",
      "Hence the models are significantly different statistically\n"
     ]
    }
   ],
   "source": [
    "max_accuracy = max(accuracies_values)\n",
    "min_accuracy = min(accuracies_values)\n",
    "best_model = accuracies_values.index(max_accuracy)\n",
    "worst_model = accuracies_values.index(min_accuracy)\n",
    "stddev = np.std(accuracies_values)\n",
    "mean = np.mean(accuracies_values)\n",
    "diff = max_accuracy - min_accuracy\n",
    "print(f\"Best Model: ({labels_graph[best_model]}) with accuracy: {max_accuracy:.2f}%\")\n",
    "print(f\"Worst Model: ({labels_graph[worst_model]}) with accuracy: {min_accuracy:.2f}%\")\n",
    "print(f\"Difference in accuracies: {diff:.2f}%\")\n",
    "print(f\"Standard Deviation in accuracies: {stddev:.2f}%\")\n",
    "sig = diff/stddev\n",
    "print(f\"The difference is equivalant to {sig:.2f} standard deviations\")\n",
    "print(\"Hence the models are significantly different statistically\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
